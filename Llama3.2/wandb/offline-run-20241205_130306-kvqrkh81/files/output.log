{'loss': 2.0072, 'grad_norm': 0.7328072786331177, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 1.658018946647644, 'eval_runtime': 6.8823, 'eval_samples_per_second': 12.641, 'eval_steps_per_second': 0.872, 'epoch': 0.22}
{'loss': 1.5866, 'grad_norm': 0.5807984471321106, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 1.4524074792861938, 'eval_runtime': 7.0612, 'eval_samples_per_second': 12.321, 'eval_steps_per_second': 0.85, 'epoch': 0.44}
{'loss': 1.5491, 'grad_norm': 0.6233791708946228, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 1.3967608213424683, 'eval_runtime': 7.1368, 'eval_samples_per_second': 12.19, 'eval_steps_per_second': 0.841, 'epoch': 0.67}
{'loss': 1.5253, 'grad_norm': 0.5286185145378113, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 1.396024465560913, 'eval_runtime': 7.1752, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 1.3824, 'grad_norm': 0.5369020104408264, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 1.4158662557601929, 'eval_runtime': 7.1853, 'eval_samples_per_second': 12.108, 'eval_steps_per_second': 0.835, 'epoch': 1.11}
{'loss': 1.2679, 'grad_norm': 0.6080968379974365, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 1.4185646772384644, 'eval_runtime': 7.1736, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 1.2261, 'grad_norm': 0.6043237447738647, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 1.4385267496109009, 'eval_runtime': 7.1951, 'eval_samples_per_second': 12.092, 'eval_steps_per_second': 0.834, 'epoch': 1.56}
{'loss': 1.2789, 'grad_norm': 0.605812668800354, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 1.448287844657898, 'eval_runtime': 7.1846, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 1.78}
{'loss': 1.1737, 'grad_norm': 0.5786942839622498, 'learning_rate': 0.003, 'epoch': 2.0}
{'eval_loss': 1.428492784500122, 'eval_runtime': 7.1648, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.9509, 'grad_norm': 0.6506620645523071, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 1.519670844078064, 'eval_runtime': 7.1881, 'eval_samples_per_second': 12.103, 'eval_steps_per_second': 0.835, 'epoch': 2.22}
{'loss': 0.924, 'grad_norm': 0.6178975701332092, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 1.5420103073120117, 'eval_runtime': 7.1694, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.9233, 'grad_norm': 0.7628898620605469, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 1.5395011901855469, 'eval_runtime': 7.1757, 'eval_samples_per_second': 12.124, 'eval_steps_per_second': 0.836, 'epoch': 2.67}
{'loss': 0.9293, 'grad_norm': 0.7347183227539062, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 1.5364874601364136, 'eval_runtime': 7.184, 'eval_samples_per_second': 12.11, 'eval_steps_per_second': 0.835, 'epoch': 2.89}
{'loss': 0.8085, 'grad_norm': 0.624043881893158, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 1.6923950910568237, 'eval_runtime': 7.1642, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 3.11}
{'loss': 0.7049, 'grad_norm': 0.7318060994148254, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 1.668001413345337, 'eval_runtime': 7.1554, 'eval_samples_per_second': 12.159, 'eval_steps_per_second': 0.839, 'epoch': 3.33}
{'loss': 0.6666, 'grad_norm': 0.6797038316726685, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
{'eval_loss': 1.6796427965164185, 'eval_runtime': 7.1834, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 3.56}
{'loss': 0.6584, 'grad_norm': 0.7391507625579834, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 1.6535953283309937, 'eval_runtime': 7.1931, 'eval_samples_per_second': 12.095, 'eval_steps_per_second': 0.834, 'epoch': 3.78}
{'loss': 0.6658, 'grad_norm': 0.6592212319374084, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 1.6795330047607422, 'eval_runtime': 7.1617, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 4.0}
{'loss': 0.4845, 'grad_norm': 0.785285234451294, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 1.8016077280044556, 'eval_runtime': 7.1731, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 4.22}
{'loss': 0.4692, 'grad_norm': 0.812506377696991, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 1.8305344581604004, 'eval_runtime': 7.1843, 'eval_samples_per_second': 12.11, 'eval_steps_per_second': 0.835, 'epoch': 4.44}
{'loss': 0.4762, 'grad_norm': 0.7182356715202332, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 1.8085330724716187, 'eval_runtime': 7.1847, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 4.67}
{'loss': 0.4585, 'grad_norm': 0.5937116742134094, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 1.8178257942199707, 'eval_runtime': 7.1775, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 4.89}
{'train_runtime': 936.8349, 'train_samples_per_second': 3.843, 'train_steps_per_second': 0.24, 'train_loss': 0.9926901520623101, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd0.1...
Fine-tuning for learning_rate=0.005 and weight_decay=0.1 complete. Model saved to './LlamaFinetuned_lr0.005_wd0.1'.
Starting fine-tuning for learning_rate=0.005 and weight_decay=0.05...
{'loss': 0.8691, 'grad_norm': 1.3134005069732666, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 1.8424503803253174, 'eval_runtime': 7.1461, 'eval_samples_per_second': 12.175, 'eval_steps_per_second': 0.84, 'epoch': 0.22}
{'loss': 0.8884, 'grad_norm': 1.024572730064392, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 1.6779996156692505, 'eval_runtime': 7.181, 'eval_samples_per_second': 12.115, 'eval_steps_per_second': 0.836, 'epoch': 0.44}
{'loss': 0.9212, 'grad_norm': 0.9734619855880737, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 1.6065247058868408, 'eval_runtime': 7.1821, 'eval_samples_per_second': 12.113, 'eval_steps_per_second': 0.835, 'epoch': 0.67}
{'loss': 0.9686, 'grad_norm': 0.9142628908157349, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 1.6108100414276123, 'eval_runtime': 7.1753, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 0.835, 'grad_norm': 0.9502301216125488, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 1.761210322380066, 'eval_runtime': 7.181, 'eval_samples_per_second': 12.115, 'eval_steps_per_second': 0.836, 'epoch': 1.11}
{'loss': 0.7038, 'grad_norm': 0.852985680103302, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 1.7876895666122437, 'eval_runtime': 7.1793, 'eval_samples_per_second': 12.118, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.6901, 'grad_norm': 0.8083502650260925, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 1.7487763166427612, 'eval_runtime': 7.1872, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 0.835, 'epoch': 1.56}
{'loss': 0.7179, 'grad_norm': 0.8695244193077087, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 1.7381393909454346, 'eval_runtime': 7.1673, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 1.78}
{'loss': 0.6917, 'grad_norm': 0.8098312616348267, 'learning_rate': 0.003, 'epoch': 2.0}
{'eval_loss': 1.7061353921890259, 'eval_runtime': 7.1805, 'eval_samples_per_second': 12.116, 'eval_steps_per_second': 0.836, 'epoch': 2.0}
{'loss': 0.5057, 'grad_norm': 0.7703126072883606, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 1.8637248277664185, 'eval_runtime': 7.1872, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 0.835, 'epoch': 2.22}
{'loss': 0.5057, 'grad_norm': 0.8004320859909058, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 1.905657410621643, 'eval_runtime': 7.171, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.5112, 'grad_norm': 0.8327847719192505, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 1.845158576965332, 'eval_runtime': 7.1933, 'eval_samples_per_second': 12.095, 'eval_steps_per_second': 0.834, 'epoch': 2.67}
{'loss': 0.5055, 'grad_norm': 0.8190730810165405, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 1.8917694091796875, 'eval_runtime': 7.1971, 'eval_samples_per_second': 12.088, 'eval_steps_per_second': 0.834, 'epoch': 2.89}
{'loss': 0.436, 'grad_norm': 0.5837844610214233, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 2.0172410011291504, 'eval_runtime': 7.2172, 'eval_samples_per_second': 12.055, 'eval_steps_per_second': 0.831, 'epoch': 3.11}
{'loss': 0.3679, 'grad_norm': 0.7104492783546448, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 1.9751627445220947, 'eval_runtime': 7.2034, 'eval_samples_per_second': 12.078, 'eval_steps_per_second': 0.833, 'epoch': 3.33}
{'loss': 0.348, 'grad_norm': 0.626211941242218, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
{'eval_loss': 2.037508249282837, 'eval_runtime': 7.1887, 'eval_samples_per_second': 12.102, 'eval_steps_per_second': 0.835, 'epoch': 3.56}
{'loss': 0.3391, 'grad_norm': 0.7081583142280579, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 2.052408218383789, 'eval_runtime': 7.1788, 'eval_samples_per_second': 12.119, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.3562, 'grad_norm': 0.6300767660140991, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 1.986826777458191, 'eval_runtime': 7.1498, 'eval_samples_per_second': 12.168, 'eval_steps_per_second': 0.839, 'epoch': 4.0}
{'loss': 0.2451, 'grad_norm': 0.6170697212219238, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 2.217789649963379, 'eval_runtime': 7.1671, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 4.22}
{'loss': 0.2379, 'grad_norm': 0.5905159115791321, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 2.1710379123687744, 'eval_runtime': 7.1704, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.254, 'grad_norm': 0.6157405376434326, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 2.1815879344940186, 'eval_runtime': 7.1518, 'eval_samples_per_second': 12.165, 'eval_steps_per_second': 0.839, 'epoch': 4.67}
{'loss': 0.2411, 'grad_norm': 0.7050321102142334, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 2.1924500465393066, 'eval_runtime': 7.1897, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 4.89}
{'train_runtime': 936.0179, 'train_samples_per_second': 3.846, 'train_steps_per_second': 0.24, 'train_loss': 0.5447033580144246, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd0.05...
Fine-tuning for learning_rate=0.005 and weight_decay=0.05 complete. Model saved to './LlamaFinetuned_lr0.005_wd0.05'.
Starting fine-tuning for learning_rate=0.005 and weight_decay=0.01...
{'loss': 0.5768, 'grad_norm': 1.1779859066009521, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 1.953072428703308, 'eval_runtime': 7.1719, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 0.22}
{'loss': 0.6511, 'grad_norm': 1.895883560180664, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 1.9075291156768799, 'eval_runtime': 7.191, 'eval_samples_per_second': 12.099, 'eval_steps_per_second': 0.834, 'epoch': 0.44}
{'loss': 0.7098, 'grad_norm': 1.1127581596374512, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 1.7902737855911255, 'eval_runtime': 7.1928, 'eval_samples_per_second': 12.095, 'eval_steps_per_second': 0.834, 'epoch': 0.67}
{'loss': 0.7542, 'grad_norm': 1.1306819915771484, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 1.7604902982711792, 'eval_runtime': 7.2019, 'eval_samples_per_second': 12.08, 'eval_steps_per_second': 0.833, 'epoch': 0.89}
{'loss': 0.6379, 'grad_norm': 1.022853970527649, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 1.9330898523330688, 'eval_runtime': 7.1826, 'eval_samples_per_second': 12.113, 'eval_steps_per_second': 0.835, 'epoch': 1.11}
{'loss': 0.5403, 'grad_norm': 1.0346304178237915, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 1.8984416723251343, 'eval_runtime': 7.1954, 'eval_samples_per_second': 12.091, 'eval_steps_per_second': 0.834, 'epoch': 1.33}
{'loss': 0.5392, 'grad_norm': 0.975818932056427, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 1.918162226676941, 'eval_runtime': 7.205, 'eval_samples_per_second': 12.075, 'eval_steps_per_second': 0.833, 'epoch': 1.56}
{'loss': 0.569, 'grad_norm': 1.0103451013565063, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 1.9149761199951172, 'eval_runtime': 7.1923, 'eval_samples_per_second': 12.096, 'eval_steps_per_second': 0.834, 'epoch': 1.78}
{'loss': 0.5669, 'grad_norm': 0.8843572735786438, 'learning_rate': 0.003, 'epoch': 2.0}
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [02:59<11:55,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:11<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:15<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:27<02:39,  3.54s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:32<00:00,  4.15s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:56,  3.98s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:30<02:38,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:36<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:57,  3.99s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:15<07:57,  3.54s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:20<05:57,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
{'eval_loss': 1.8661922216415405, 'eval_runtime': 7.1953, 'eval_samples_per_second': 12.091, 'eval_steps_per_second': 0.834, 'epoch': 2.0}
{'loss': 0.3864, 'grad_norm': 0.809306263923645, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 1.9652732610702515, 'eval_runtime': 7.1917, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 2.22}
{'loss': 0.4003, 'grad_norm': 1.0479103326797485, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 2.0145514011383057, 'eval_runtime': 7.1989, 'eval_samples_per_second': 12.085, 'eval_steps_per_second': 0.833, 'epoch': 2.44}
{'loss': 0.3995, 'grad_norm': 0.8738089203834534, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 2.0279223918914795, 'eval_runtime': 7.1917, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 2.67}
{'loss': 0.3886, 'grad_norm': 0.7960625290870667, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 1.9821536540985107, 'eval_runtime': 7.1801, 'eval_samples_per_second': 12.117, 'eval_steps_per_second': 0.836, 'epoch': 2.89}
{'loss': 0.3383, 'grad_norm': 0.6110437512397766, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 2.107919216156006, 'eval_runtime': 7.1815, 'eval_samples_per_second': 12.114, 'eval_steps_per_second': 0.835, 'epoch': 3.11}
{'loss': 0.2821, 'grad_norm': 0.7020478844642639, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 2.109497308731079, 'eval_runtime': 7.1919, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 3.33}
{'loss': 0.2707, 'grad_norm': 0.7603546977043152, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
{'eval_loss': 2.1646692752838135, 'eval_runtime': 7.2125, 'eval_samples_per_second': 12.062, 'eval_steps_per_second': 0.832, 'epoch': 3.56}
{'loss': 0.2638, 'grad_norm': 0.6028473377227783, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 2.1003952026367188, 'eval_runtime': 7.1894, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 3.78}
{'loss': 0.2728, 'grad_norm': 0.5703292489051819, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 2.123143434524536, 'eval_runtime': 7.1782, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 0.836, 'epoch': 4.0}
{'loss': 0.1838, 'grad_norm': 0.5381303429603577, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 2.278729200363159, 'eval_runtime': 7.172, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 4.22}
{'loss': 0.1889, 'grad_norm': 0.5544969439506531, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 2.265554904937744, 'eval_runtime': 7.1854, 'eval_samples_per_second': 12.108, 'eval_steps_per_second': 0.835, 'epoch': 4.44}
{'loss': 0.1985, 'grad_norm': 0.5364884734153748, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 2.2742223739624023, 'eval_runtime': 7.181, 'eval_samples_per_second': 12.115, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.1848, 'grad_norm': 0.48447301983833313, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 2.284910202026367, 'eval_runtime': 7.1783, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 0.836, 'epoch': 4.89}
{'train_runtime': 937.6135, 'train_samples_per_second': 3.84, 'train_steps_per_second': 0.24, 'train_loss': 0.4175518470340305, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd0.01...
Fine-tuning for learning_rate=0.005 and weight_decay=0.01 complete. Model saved to './LlamaFinetuned_lr0.005_wd0.01'.
Starting fine-tuning for learning_rate=0.005 and weight_decay=0.005...
{'loss': 0.4202, 'grad_norm': 1.1880038976669312, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 2.06742262840271, 'eval_runtime': 7.1568, 'eval_samples_per_second': 12.156, 'eval_steps_per_second': 0.838, 'epoch': 0.22}
{'loss': 0.5633, 'grad_norm': 1.1937072277069092, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 1.9745482206344604, 'eval_runtime': 7.1937, 'eval_samples_per_second': 12.094, 'eval_steps_per_second': 0.834, 'epoch': 0.44}
{'loss': 0.6014, 'grad_norm': 1.3841521739959717, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 1.8782485723495483, 'eval_runtime': 7.1866, 'eval_samples_per_second': 12.106, 'eval_steps_per_second': 0.835, 'epoch': 0.67}
{'loss': 0.6571, 'grad_norm': 1.1621475219726562, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 1.8740220069885254, 'eval_runtime': 7.1798, 'eval_samples_per_second': 12.117, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 0.5523, 'grad_norm': 1.0710985660552979, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 1.9852293729782104, 'eval_runtime': 7.1809, 'eval_samples_per_second': 12.115, 'eval_steps_per_second': 0.836, 'epoch': 1.11}
{'loss': 0.4593, 'grad_norm': 1.2355144023895264, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 2.017490863800049, 'eval_runtime': 7.1634, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
{'loss': 0.4567, 'grad_norm': 0.904819130897522, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 2.009448766708374, 'eval_runtime': 7.184, 'eval_samples_per_second': 12.11, 'eval_steps_per_second': 0.835, 'epoch': 1.56}
{'loss': 0.4837, 'grad_norm': 1.1745392084121704, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 1.989406704902649, 'eval_runtime': 7.1992, 'eval_samples_per_second': 12.085, 'eval_steps_per_second': 0.833, 'epoch': 1.78}
{'loss': 0.479, 'grad_norm': 1.1050498485565186, 'learning_rate': 0.003, 'epoch': 2.0}
{'eval_loss': 1.9397571086883545, 'eval_runtime': 7.1868, 'eval_samples_per_second': 12.106, 'eval_steps_per_second': 0.835, 'epoch': 2.0}
{'loss': 0.3289, 'grad_norm': 0.8277505040168762, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 2.079993486404419, 'eval_runtime': 7.1719, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.3456, 'grad_norm': 0.7894899845123291, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 2.0812430381774902, 'eval_runtime': 7.1775, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 2.44}
{'loss': 0.3545, 'grad_norm': 0.9170963764190674, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 2.062227964401245, 'eval_runtime': 7.1828, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 2.67}
{'loss': 0.3451, 'grad_norm': 0.83283531665802, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 2.0630245208740234, 'eval_runtime': 7.1659, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.2843, 'grad_norm': 0.6415544152259827, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 2.177018642425537, 'eval_runtime': 7.1394, 'eval_samples_per_second': 12.186, 'eval_steps_per_second': 0.84, 'epoch': 3.11}
{'loss': 0.2432, 'grad_norm': 0.688687801361084, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 2.214587450027466, 'eval_runtime': 7.1747, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 3.33}
{'loss': 0.224, 'grad_norm': 0.6768162846565247, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
{'eval_loss': 2.2768797874450684, 'eval_runtime': 7.135, 'eval_samples_per_second': 12.193, 'eval_steps_per_second': 0.841, 'epoch': 3.56}
{'loss': 0.2226, 'grad_norm': 0.49584153294563293, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 2.209719181060791, 'eval_runtime': 7.1655, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.2306, 'grad_norm': 0.5506696105003357, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 2.213602066040039, 'eval_runtime': 7.1625, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 4.0}
{'loss': 0.1618, 'grad_norm': 0.5831854939460754, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 2.3414108753204346, 'eval_runtime': 7.1601, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.1591, 'grad_norm': 0.6456986665725708, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 2.3703858852386475, 'eval_runtime': 7.1405, 'eval_samples_per_second': 12.184, 'eval_steps_per_second': 0.84, 'epoch': 4.44}
{'loss': 0.1691, 'grad_norm': 0.5966911315917969, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 2.3501574993133545, 'eval_runtime': 7.1624, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 4.67}
{'loss': 0.1637, 'grad_norm': 0.3939366638660431, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 2.356278419494629, 'eval_runtime': 7.1407, 'eval_samples_per_second': 12.184, 'eval_steps_per_second': 0.84, 'epoch': 4.89}
{'train_runtime': 935.8062, 'train_samples_per_second': 3.847, 'train_steps_per_second': 0.24, 'train_loss': 0.3548891091346741, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd0.005...
Fine-tuning for learning_rate=0.005 and weight_decay=0.005 complete. Model saved to './LlamaFinetuned_lr0.005_wd0.005'.
Starting fine-tuning for learning_rate=0.005 and weight_decay=0.001...
{'loss': 0.388, 'grad_norm': 1.2484763860702515, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 2.0409815311431885, 'eval_runtime': 7.1291, 'eval_samples_per_second': 12.203, 'eval_steps_per_second': 0.842, 'epoch': 0.22}
{'loss': 0.5223, 'grad_norm': 1.316393256187439, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 2.079192876815796, 'eval_runtime': 7.1655, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 0.44}
{'loss': 0.5579, 'grad_norm': 1.4201281070709229, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 2.0184528827667236, 'eval_runtime': 7.1745, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 0.67}
{'loss': 0.6119, 'grad_norm': 1.1406207084655762, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 1.9258407354354858, 'eval_runtime': 7.1711, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 0.89}
{'loss': 0.4944, 'grad_norm': 1.2428162097930908, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 2.1040828227996826, 'eval_runtime': 7.1478, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 0.839, 'epoch': 1.11}
{'loss': 0.4189, 'grad_norm': 1.1184399127960205, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 2.102255344390869, 'eval_runtime': 7.1774, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.4351, 'grad_norm': 1.1429041624069214, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 2.09431529045105, 'eval_runtime': 7.1714, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 1.56}
{'loss': 0.4561, 'grad_norm': 1.1805123090744019, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 2.0324878692626953, 'eval_runtime': 7.1943, 'eval_samples_per_second': 12.093, 'eval_steps_per_second': 0.834, 'epoch': 1.78}
{'loss': 0.4456, 'grad_norm': 1.1770371198654175, 'learning_rate': 0.003, 'epoch': 2.0}
{'eval_loss': 2.0133676528930664, 'eval_runtime': 7.1893, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 2.0}
{'loss': 0.3151, 'grad_norm': 0.8636904358863831, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 2.1352481842041016, 'eval_runtime': 7.1716, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.3098, 'grad_norm': 0.8914275765419006, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 2.1313140392303467, 'eval_runtime': 7.1522, 'eval_samples_per_second': 12.164, 'eval_steps_per_second': 0.839, 'epoch': 2.44}
{'loss': 0.3233, 'grad_norm': 0.8844184279441833, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 2.138450860977173, 'eval_runtime': 7.186, 'eval_samples_per_second': 12.107, 'eval_steps_per_second': 0.835, 'epoch': 2.67}
{'loss': 0.318, 'grad_norm': 0.8549925088882446, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 2.1514949798583984, 'eval_runtime': 7.1724, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.2703, 'grad_norm': 0.6198544502258301, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 2.2203540802001953, 'eval_runtime': 7.192, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 3.11}
{'loss': 0.2192, 'grad_norm': 0.6212027072906494, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 2.3088488578796387, 'eval_runtime': 7.1953, 'eval_samples_per_second': 12.091, 'eval_steps_per_second': 0.834, 'epoch': 3.33}
{'loss': 0.2166, 'grad_norm': 0.6216878890991211, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
{'eval_loss': 2.2503421306610107, 'eval_runtime': 7.1642, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.837, 'epoch': 3.56}
{'loss': 0.2124, 'grad_norm': 0.637474536895752, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 2.2832705974578857, 'eval_runtime': 7.1841, 'eval_samples_per_second': 12.11, 'eval_steps_per_second': 0.835, 'epoch': 3.78}
{'loss': 0.2194, 'grad_norm': 0.5178130269050598, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 2.2188146114349365, 'eval_runtime': 7.1653, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 4.0}
{'loss': 0.1517, 'grad_norm': 0.4653041958808899, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 2.3368844985961914, 'eval_runtime': 7.1748, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 4.22}
{'loss': 0.1554, 'grad_norm': 0.5382931232452393, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 2.3863978385925293, 'eval_runtime': 7.1399, 'eval_samples_per_second': 12.185, 'eval_steps_per_second': 0.84, 'epoch': 4.44}
{'loss': 0.1594, 'grad_norm': 0.42821741104125977, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 2.3733103275299072, 'eval_runtime': 7.1703, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 4.67}
{'loss': 0.1539, 'grad_norm': 0.4092824161052704, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 2.3681771755218506, 'eval_runtime': 7.1619, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 4.89}
{'train_runtime': 935.0648, 'train_samples_per_second': 3.85, 'train_steps_per_second': 0.241, 'train_loss': 0.3303114268514845, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd0.001...
Fine-tuning for learning_rate=0.005 and weight_decay=0.001 complete. Model saved to './LlamaFinetuned_lr0.005_wd0.001'.
Starting fine-tuning for learning_rate=0.005 and weight_decay=0.0005...
{'loss': 0.3458, 'grad_norm': 1.1137760877609253, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 2.1275739669799805, 'eval_runtime': 7.1482, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.4884, 'grad_norm': 1.4435724020004272, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 2.03969669342041, 'eval_runtime': 7.1748, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 0.44}
{'loss': 0.5383, 'grad_norm': 1.461164116859436, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 2.0243632793426514, 'eval_runtime': 7.1827, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 0.67}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:31<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:37<00:00,  4.17s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:56,  3.98s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:15<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:30<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:35<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:55,  3.98s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:29<02:38,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:35<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:55,  3.98s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 22%|███████████████████████████████▎                                                                                                             | 50/225 [03:21<10:24,  3.57s/it]
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:30<02:39,  3.54s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:36<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:56,  3.98s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:15<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:30<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:36<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:53,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:29<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 89%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍               | 200/225 [13:46<01:27,  3.51s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:06<00:00,  1.12s/it]
{'loss': 0.5647, 'grad_norm': 1.3595044612884521, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 1.9724754095077515, 'eval_runtime': 7.1629, 'eval_samples_per_second': 12.146, 'eval_steps_per_second': 0.838, 'epoch': 0.89}
{'loss': 0.4804, 'grad_norm': 0.980146586894989, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 2.078620433807373, 'eval_runtime': 7.1788, 'eval_samples_per_second': 12.119, 'eval_steps_per_second': 0.836, 'epoch': 1.11}
{'loss': 0.3965, 'grad_norm': 1.209015130996704, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 2.0964365005493164, 'eval_runtime': 7.1556, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.839, 'epoch': 1.33}
{'loss': 0.4056, 'grad_norm': 1.0428332090377808, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 2.125561237335205, 'eval_runtime': 7.1917, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 1.56}
{'loss': 0.4188, 'grad_norm': 1.3373101949691772, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 2.1223745346069336, 'eval_runtime': 7.1386, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.84, 'epoch': 1.78}
{'loss': 0.4182, 'grad_norm': 1.1755770444869995, 'learning_rate': 0.003, 'epoch': 2.0}
{'eval_loss': 2.058171510696411, 'eval_runtime': 7.1917, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 2.0}
{'loss': 0.2907, 'grad_norm': 0.8300073146820068, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 2.1863884925842285, 'eval_runtime': 7.1716, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.2915, 'grad_norm': 0.8973153233528137, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 2.2066032886505127, 'eval_runtime': 7.1774, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 2.44}
{'loss': 0.2992, 'grad_norm': 0.7490842938423157, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 2.1703925132751465, 'eval_runtime': 7.1718, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.2933, 'grad_norm': 0.8373844027519226, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 2.1827003955841064, 'eval_runtime': 7.1715, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.2439, 'grad_norm': 0.7620350122451782, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 2.269829273223877, 'eval_runtime': 7.1714, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 3.11}
{'loss': 0.2083, 'grad_norm': 0.7496116161346436, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 2.2819693088531494, 'eval_runtime': 7.1634, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 3.33}
{'loss': 0.2024, 'grad_norm': 0.6780669689178467, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
{'eval_loss': 2.3213279247283936, 'eval_runtime': 7.2038, 'eval_samples_per_second': 12.077, 'eval_steps_per_second': 0.833, 'epoch': 3.56}
{'loss': 0.1971, 'grad_norm': 0.8047543168067932, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 2.296421766281128, 'eval_runtime': 7.167, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.1991, 'grad_norm': 0.5766345858573914, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 2.276975154876709, 'eval_runtime': 7.1865, 'eval_samples_per_second': 12.106, 'eval_steps_per_second': 0.835, 'epoch': 4.0}
{'loss': 0.1433, 'grad_norm': 0.5807498097419739, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 2.366629123687744, 'eval_runtime': 7.1584, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.1467, 'grad_norm': 0.475092351436615, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 2.3871817588806152, 'eval_runtime': 7.1802, 'eval_samples_per_second': 12.117, 'eval_steps_per_second': 0.836, 'epoch': 4.44}
{'loss': 0.1514, 'grad_norm': 0.38685423135757446, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 2.3854148387908936, 'eval_runtime': 7.1699, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 0.837, 'epoch': 4.67}
{'loss': 0.1506, 'grad_norm': 0.5201559662818909, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 2.376652479171753, 'eval_runtime': 7.1725, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 4.89}
{'train_runtime': 936.0226, 'train_samples_per_second': 3.846, 'train_steps_per_second': 0.24, 'train_loss': 0.3087528853946262, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd0.0005...
Fine-tuning for learning_rate=0.005 and weight_decay=0.0005 complete. Model saved to './LlamaFinetuned_lr0.005_wd0.0005'.
Starting fine-tuning for learning_rate=0.005 and weight_decay=0.0001...
{'loss': 0.3201, 'grad_norm': 1.3139965534210205, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 2.135793685913086, 'eval_runtime': 7.1619, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 0.22}
{'loss': 0.4488, 'grad_norm': 1.8184053897857666, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 2.162477493286133, 'eval_runtime': 7.1858, 'eval_samples_per_second': 12.107, 'eval_steps_per_second': 0.835, 'epoch': 0.44}
{'loss': 0.5054, 'grad_norm': 1.4381715059280396, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 2.1285240650177, 'eval_runtime': 7.1893, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 0.67}
{'loss': 0.5392, 'grad_norm': 1.448447346687317, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 2.0493946075439453, 'eval_runtime': 7.1946, 'eval_samples_per_second': 12.092, 'eval_steps_per_second': 0.834, 'epoch': 0.89}
{'loss': 0.462, 'grad_norm': 1.1469122171401978, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 2.1886792182922363, 'eval_runtime': 7.1979, 'eval_samples_per_second': 12.087, 'eval_steps_per_second': 0.834, 'epoch': 1.11}
{'loss': 0.3796, 'grad_norm': 1.2605465650558472, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 2.131197452545166, 'eval_runtime': 7.1774, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.3934, 'grad_norm': 1.11121666431427, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 2.1328861713409424, 'eval_runtime': 7.1729, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 1.56}
{'loss': 0.4057, 'grad_norm': 1.1737117767333984, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 2.084355592727661, 'eval_runtime': 7.1947, 'eval_samples_per_second': 12.092, 'eval_steps_per_second': 0.834, 'epoch': 1.78}
{'loss': 0.4073, 'grad_norm': 1.2368297576904297, 'learning_rate': 0.003, 'epoch': 2.0}
{'eval_loss': 2.0843136310577393, 'eval_runtime': 7.1658, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.2795, 'grad_norm': 0.826185405254364, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 2.182720422744751, 'eval_runtime': 7.1483, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.839, 'epoch': 2.22}
{'loss': 0.2773, 'grad_norm': 0.9532471299171448, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 2.211872100830078, 'eval_runtime': 7.1653, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.2828, 'grad_norm': 0.9252888560295105, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 2.257194995880127, 'eval_runtime': 7.1751, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 2.67}
{'loss': 0.2875, 'grad_norm': 0.8374130129814148, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 2.198223352432251, 'eval_runtime': 7.1769, 'eval_samples_per_second': 12.122, 'eval_steps_per_second': 0.836, 'epoch': 2.89}
{'loss': 0.2465, 'grad_norm': 0.7525762319564819, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 2.2692830562591553, 'eval_runtime': 7.1536, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 0.839, 'epoch': 3.11}
{'loss': 0.2004, 'grad_norm': 0.7186568379402161, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 2.321582317352295, 'eval_runtime': 7.159, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 3.33}
{'loss': 0.1963, 'grad_norm': 0.6319475173950195, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
{'eval_loss': 2.3380634784698486, 'eval_runtime': 7.161, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.1915, 'grad_norm': 0.5492706298828125, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 2.2815141677856445, 'eval_runtime': 7.1682, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.1961, 'grad_norm': 0.6140271425247192, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 2.288357734680176, 'eval_runtime': 7.1645, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 4.0}
{'loss': 0.1367, 'grad_norm': 0.42339321970939636, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 2.395130157470703, 'eval_runtime': 7.171, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 4.22}
{'loss': 0.1402, 'grad_norm': 0.48541516065597534, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 2.416602849960327, 'eval_runtime': 7.1966, 'eval_samples_per_second': 12.089, 'eval_steps_per_second': 0.834, 'epoch': 4.44}
{'loss': 0.1452, 'grad_norm': 0.3319973051548004, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 2.4105451107025146, 'eval_runtime': 7.1732, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.1422, 'grad_norm': 0.3631930649280548, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 2.4129271507263184, 'eval_runtime': 7.1815, 'eval_samples_per_second': 12.114, 'eval_steps_per_second': 0.835, 'epoch': 4.89}
{'train_runtime': 936.189, 'train_samples_per_second': 3.845, 'train_steps_per_second': 0.24, 'train_loss': 0.29571268929375544, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd0.0001...
Fine-tuning for learning_rate=0.005 and weight_decay=0.0001 complete. Model saved to './LlamaFinetuned_lr0.005_wd0.0001'.
Starting fine-tuning for learning_rate=0.005 and weight_decay=1e-05...
{'loss': 0.3505, 'grad_norm': 1.508504867553711, 'learning_rate': 0.004777777777777778, 'epoch': 0.22}
{'eval_loss': 2.142493724822998, 'eval_runtime': 7.1472, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.4911, 'grad_norm': 1.9882521629333496, 'learning_rate': 0.004555555555555556, 'epoch': 0.44}
{'eval_loss': 2.2257707118988037, 'eval_runtime': 7.1682, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 0.44}
{'loss': 0.4895, 'grad_norm': 1.5176095962524414, 'learning_rate': 0.004333333333333334, 'epoch': 0.67}
{'eval_loss': 2.076071262359619, 'eval_runtime': 7.162, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 0.67}
{'loss': 0.5338, 'grad_norm': 1.2595735788345337, 'learning_rate': 0.004111111111111111, 'epoch': 0.89}
{'eval_loss': 2.0220062732696533, 'eval_runtime': 7.1465, 'eval_samples_per_second': 12.174, 'eval_steps_per_second': 0.84, 'epoch': 0.89}
{'loss': 0.4393, 'grad_norm': 1.070220708847046, 'learning_rate': 0.003888888888888889, 'epoch': 1.11}
{'eval_loss': 2.1481261253356934, 'eval_runtime': 7.1471, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.84, 'epoch': 1.11}
{'loss': 0.3826, 'grad_norm': 1.280300498008728, 'learning_rate': 0.0036666666666666666, 'epoch': 1.33}
{'eval_loss': 2.147695779800415, 'eval_runtime': 7.1796, 'eval_samples_per_second': 12.118, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.3894, 'grad_norm': 1.1298614740371704, 'learning_rate': 0.0034444444444444444, 'epoch': 1.56}
{'eval_loss': 2.149893045425415, 'eval_runtime': 7.153, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 1.56}
{'loss': 0.395, 'grad_norm': 1.2224808931350708, 'learning_rate': 0.0032222222222222227, 'epoch': 1.78}
{'eval_loss': 2.100945234298706, 'eval_runtime': 7.1781, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 0.836, 'epoch': 1.78}
{'loss': 0.392, 'grad_norm': 1.1295125484466553, 'learning_rate': 0.003, 'epoch': 2.0}
{'eval_loss': 2.1465184688568115, 'eval_runtime': 7.1767, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 2.0}
{'loss': 0.2779, 'grad_norm': 0.8827656507492065, 'learning_rate': 0.002777777777777778, 'epoch': 2.22}
{'eval_loss': 2.3182778358459473, 'eval_runtime': 7.1616, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 2.22}
{'loss': 0.2849, 'grad_norm': 1.7861264944076538, 'learning_rate': 0.0025555555555555553, 'epoch': 2.44}
{'eval_loss': 2.2009835243225098, 'eval_runtime': 7.184, 'eval_samples_per_second': 12.11, 'eval_steps_per_second': 0.835, 'epoch': 2.44}
{'loss': 0.2877, 'grad_norm': 0.9292911887168884, 'learning_rate': 0.0023333333333333335, 'epoch': 2.67}
{'eval_loss': 2.267068386077881, 'eval_runtime': 7.1473, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 0.839, 'epoch': 2.67}
{'loss': 0.2856, 'grad_norm': 0.7761551737785339, 'learning_rate': 0.0021111111111111113, 'epoch': 2.89}
{'eval_loss': 2.199843406677246, 'eval_runtime': 7.1829, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 2.89}
{'loss': 0.2404, 'grad_norm': 0.6657603979110718, 'learning_rate': 0.001888888888888889, 'epoch': 3.11}
{'eval_loss': 2.3380627632141113, 'eval_runtime': 7.1607, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 3.11}
{'loss': 0.2055, 'grad_norm': 0.6516525149345398, 'learning_rate': 0.0016666666666666666, 'epoch': 3.33}
{'eval_loss': 2.3380494117736816, 'eval_runtime': 7.175, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 3.33}
{'loss': 0.1897, 'grad_norm': 0.7661834955215454, 'learning_rate': 0.0014444444444444444, 'epoch': 3.56}
  return fn(*args, **kwargs)                                                                                                                                                       .46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:02<11:53,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:13<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:17<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:27<02:38,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:33<00:00,  4.15s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:55,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:29<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)                                                                                                                                                       .46 of 🤗 Transformers. Use `eval_strategy` instead
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:34<00:00,  4.15s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 16%|███████████████████████▏                                                                                                                     | 37/225 [02:28<11:35,  3.70s/it]
{'loss': 0.1856, 'grad_norm': 0.5881131291389465, 'learning_rate': 0.0012222222222222222, 'epoch': 3.78}
{'eval_loss': 2.344446897506714, 'eval_runtime': 7.1599, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 0.838, 'epoch': 3.78}
{'loss': 0.1947, 'grad_norm': 0.5956440567970276, 'learning_rate': 0.001, 'epoch': 4.0}
{'eval_loss': 2.3191752433776855, 'eval_runtime': 7.1455, 'eval_samples_per_second': 12.176, 'eval_steps_per_second': 0.84, 'epoch': 4.0}
{'loss': 0.1389, 'grad_norm': 0.47169649600982666, 'learning_rate': 0.0007777777777777778, 'epoch': 4.22}
{'eval_loss': 2.4276275634765625, 'eval_runtime': 7.1539, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 4.22}
{'loss': 0.1386, 'grad_norm': 0.5698711276054382, 'learning_rate': 0.0005555555555555556, 'epoch': 4.44}
{'eval_loss': 2.45560359954834, 'eval_runtime': 7.1702, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.1456, 'grad_norm': 0.395504891872406, 'learning_rate': 0.0003333333333333333, 'epoch': 4.67}
{'eval_loss': 2.431886672973633, 'eval_runtime': 7.1655, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 4.67}
{'loss': 0.1415, 'grad_norm': 0.4076184332370758, 'learning_rate': 0.00011111111111111112, 'epoch': 4.89}
{'eval_loss': 2.4356496334075928, 'eval_runtime': 7.1537, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 0.839, 'epoch': 4.89}
{'train_runtime': 934.9352, 'train_samples_per_second': 3.851, 'train_steps_per_second': 0.241, 'train_loss': 0.2954685820473565, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.005_wd1e-05...
Fine-tuning for learning_rate=0.005 and weight_decay=1e-05 complete. Model saved to './LlamaFinetuned_lr0.005_wd1e-05'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=0.1...
{'loss': 0.1094, 'grad_norm': 0.42311856150627136, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.464621067047119, 'eval_runtime': 7.1479, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.1381, 'grad_norm': 0.48483189940452576, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.4664151668548584, 'eval_runtime': 7.1472, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.839, 'epoch': 0.44}
{'loss': 0.1368, 'grad_norm': 0.5250903367996216, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.4311673641204834, 'eval_runtime': 7.1285, 'eval_samples_per_second': 12.205, 'eval_steps_per_second': 0.842, 'epoch': 0.67}
{'loss': 0.1397, 'grad_norm': 0.40790674090385437, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.4820942878723145, 'eval_runtime': 7.1634, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 0.89}
{'loss': 0.1213, 'grad_norm': 0.3233134150505066, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
{'eval_loss': 2.4611387252807617, 'eval_runtime': 7.1434, 'eval_samples_per_second': 12.179, 'eval_steps_per_second': 0.84, 'epoch': 1.11}
{'loss': 0.1076, 'grad_norm': 0.3729684352874756, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.5424318313598633, 'eval_runtime': 7.1669, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 1.33}
{'loss': 0.113, 'grad_norm': 0.37775370478630066, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.5958759784698486, 'eval_runtime': 7.1607, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 1.56}
{'loss': 0.1121, 'grad_norm': 0.42187151312828064, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.5355236530303955, 'eval_runtime': 7.1604, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 1.78}
{'loss': 0.1144, 'grad_norm': 0.36521920561790466, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.503089427947998, 'eval_runtime': 7.132, 'eval_samples_per_second': 12.198, 'eval_steps_per_second': 0.841, 'epoch': 2.0}
{'loss': 0.0961, 'grad_norm': 0.2452116161584854, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.5693156719207764, 'eval_runtime': 7.1444, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 2.22}
{'loss': 0.0962, 'grad_norm': 0.31615832448005676, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.618295192718506, 'eval_runtime': 7.168, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.0977, 'grad_norm': 0.2519511580467224, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.6034064292907715, 'eval_runtime': 7.1555, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.839, 'epoch': 2.67}
{'loss': 0.1018, 'grad_norm': 0.31266483664512634, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.5673646926879883, 'eval_runtime': 7.1539, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 2.89}
{'loss': 0.1018, 'grad_norm': 0.22591957449913025, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.5681474208831787, 'eval_runtime': 7.1701, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 0.837, 'epoch': 3.11}
{'loss': 0.0945, 'grad_norm': 0.2929023504257202, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.6166234016418457, 'eval_runtime': 7.1426, 'eval_samples_per_second': 12.18, 'eval_steps_per_second': 0.84, 'epoch': 3.33}
{'loss': 0.095, 'grad_norm': 0.23930135369300842, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.637958526611328, 'eval_runtime': 7.1228, 'eval_samples_per_second': 12.214, 'eval_steps_per_second': 0.842, 'epoch': 3.56}
{'loss': 0.0935, 'grad_norm': 0.2645910680294037, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.6286580562591553, 'eval_runtime': 7.1393, 'eval_samples_per_second': 12.186, 'eval_steps_per_second': 0.84, 'epoch': 3.78}
{'loss': 0.0987, 'grad_norm': 0.22370953857898712, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.606825828552246, 'eval_runtime': 7.1269, 'eval_samples_per_second': 12.207, 'eval_steps_per_second': 0.842, 'epoch': 4.0}
{'loss': 0.0905, 'grad_norm': 0.22852346301078796, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.610017776489258, 'eval_runtime': 7.1412, 'eval_samples_per_second': 12.183, 'eval_steps_per_second': 0.84, 'epoch': 4.22}
{'loss': 0.0975, 'grad_norm': 0.2716772258281708, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.6286461353302, 'eval_runtime': 7.1293, 'eval_samples_per_second': 12.203, 'eval_steps_per_second': 0.842, 'epoch': 4.44}
{'loss': 0.1023, 'grad_norm': 0.310768187046051, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.633740186691284, 'eval_runtime': 7.1524, 'eval_samples_per_second': 12.164, 'eval_steps_per_second': 0.839, 'epoch': 4.67}
{'loss': 0.1021, 'grad_norm': 0.3035968542098999, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.6310389041900635, 'eval_runtime': 7.1528, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 4.89}
{'train_runtime': 933.2297, 'train_samples_per_second': 3.858, 'train_steps_per_second': 0.241, 'train_loss': 0.10722452878952027, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd0.1...
Fine-tuning for learning_rate=0.001 and weight_decay=0.1 complete. Model saved to './LlamaFinetuned_lr0.001_wd0.1'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=0.05...
{'loss': 0.0807, 'grad_norm': 0.2674546241760254, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.5856282711029053, 'eval_runtime': 7.1617, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 0.22}
{'loss': 0.1067, 'grad_norm': 0.2536395788192749, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.6046142578125, 'eval_runtime': 7.1922, 'eval_samples_per_second': 12.096, 'eval_steps_per_second': 0.834, 'epoch': 0.44}
{'loss': 0.1056, 'grad_norm': 0.25901558995246887, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.606218099594116, 'eval_runtime': 7.18, 'eval_samples_per_second': 12.117, 'eval_steps_per_second': 0.836, 'epoch': 0.67}
{'loss': 0.1106, 'grad_norm': 0.35207703709602356, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.596709966659546, 'eval_runtime': 7.1655, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 0.89}
{'loss': 0.1005, 'grad_norm': 0.38862091302871704, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
{'eval_loss': 2.6049463748931885, 'eval_runtime': 7.1786, 'eval_samples_per_second': 12.119, 'eval_steps_per_second': 0.836, 'epoch': 1.11}
{'loss': 0.0956, 'grad_norm': 0.2938864529132843, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.652653455734253, 'eval_runtime': 7.1678, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 1.33}
{'loss': 0.1004, 'grad_norm': 0.2945062220096588, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.6751251220703125, 'eval_runtime': 7.1711, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 1.56}
{'loss': 0.1012, 'grad_norm': 0.3205834925174713, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.6099064350128174, 'eval_runtime': 7.1481, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.839, 'epoch': 1.78}
{'loss': 0.1008, 'grad_norm': 0.2355802357196808, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.602839231491089, 'eval_runtime': 7.1678, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.088, 'grad_norm': 0.20426693558692932, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.6876816749572754, 'eval_runtime': 7.1639, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 2.22}
{'loss': 0.0891, 'grad_norm': 0.19831763207912445, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.709845781326294, 'eval_runtime': 7.1566, 'eval_samples_per_second': 12.157, 'eval_steps_per_second': 0.838, 'epoch': 2.44}
{'loss': 0.0908, 'grad_norm': 0.22821196913719177, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.6938562393188477, 'eval_runtime': 7.1727, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.0939, 'grad_norm': 0.35520821809768677, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.6669082641601562, 'eval_runtime': 7.1719, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.0953, 'grad_norm': 0.258934885263443, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.685739517211914, 'eval_runtime': 7.1656, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 3.11}
{'loss': 0.0885, 'grad_norm': 0.3215823471546173, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.713489294052124, 'eval_runtime': 7.1422, 'eval_samples_per_second': 12.181, 'eval_steps_per_second': 0.84, 'epoch': 3.33}
{'loss': 0.0898, 'grad_norm': 0.22870439291000366, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.724053144454956, 'eval_runtime': 7.1652, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 3.56}
{'loss': 0.0875, 'grad_norm': 0.2706059217453003, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.713141918182373, 'eval_runtime': 7.1706, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.0926, 'grad_norm': 0.22093863785266876, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.6924026012420654, 'eval_runtime': 7.1896, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 4.0}
{'loss': 0.0859, 'grad_norm': 0.36860814690589905, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.692051649093628, 'eval_runtime': 7.1665, 'eval_samples_per_second': 12.14, 'eval_steps_per_second': 0.837, 'epoch': 4.22}
{'loss': 0.0925, 'grad_norm': 0.2587769627571106, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.708538293838501, 'eval_runtime': 7.1797, 'eval_samples_per_second': 12.118, 'eval_steps_per_second': 0.836, 'epoch': 4.44}
{'loss': 0.0966, 'grad_norm': 0.23017965257167816, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.709627628326416, 'eval_runtime': 7.1522, 'eval_samples_per_second': 12.164, 'eval_steps_per_second': 0.839, 'epoch': 4.67}
{'loss': 0.0955, 'grad_norm': 0.3463961184024811, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.706782579421997, 'eval_runtime': 7.1686, 'eval_samples_per_second': 12.136, 'eval_steps_per_second': 0.837, 'epoch': 4.89}
{'train_runtime': 934.8363, 'train_samples_per_second': 3.851, 'train_steps_per_second': 0.241, 'train_loss': 0.09503762483596802, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd0.05...
Fine-tuning for learning_rate=0.001 and weight_decay=0.05 complete. Model saved to './LlamaFinetuned_lr0.001_wd0.05'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=0.01...
{'loss': 0.0695, 'grad_norm': 0.3075754940509796, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.7403385639190674, 'eval_runtime': 7.1556, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.0929, 'grad_norm': 0.25389355421066284, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.7050719261169434, 'eval_runtime': 7.154, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 0.44}
{'loss': 0.0982, 'grad_norm': 0.3220297694206238, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.6782925128936768, 'eval_runtime': 7.1813, 'eval_samples_per_second': 12.115, 'eval_steps_per_second': 0.836, 'epoch': 0.67}
{'loss': 0.0985, 'grad_norm': 0.273103803396225, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.7089056968688965, 'eval_runtime': 7.1588, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 0.89}
{'loss': 0.0927, 'grad_norm': 0.28048965334892273, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
{'eval_loss': 2.7079596519470215, 'eval_runtime': 7.1831, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 1.11}
{'loss': 0.0876, 'grad_norm': 0.2760327458381653, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.6860640048980713, 'eval_runtime': 7.1589, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
{'loss': 0.0942, 'grad_norm': 0.2709257900714874, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.742546319961548, 'eval_runtime': 7.1612, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 1.56}
{'loss': 0.0934, 'grad_norm': 0.34441012144088745, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.7189712524414062, 'eval_runtime': 7.1644, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 1.78}
{'loss': 0.0935, 'grad_norm': 0.28683674335479736, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.715178966522217, 'eval_runtime': 7.1763, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 2.0}
{'loss': 0.0846, 'grad_norm': 0.2758202850818634, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.7576045989990234, 'eval_runtime': 7.1857, 'eval_samples_per_second': 12.107, 'eval_steps_per_second': 0.835, 'epoch': 2.22}
{'loss': 0.0841, 'grad_norm': 0.20231957733631134, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.7645890712738037, 'eval_runtime': 7.1854, 'eval_samples_per_second': 12.108, 'eval_steps_per_second': 0.835, 'epoch': 2.44}
{'loss': 0.0852, 'grad_norm': 0.23976777493953705, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.7915241718292236, 'eval_runtime': 7.1703, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.0884, 'grad_norm': 0.20835882425308228, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.756466865539551, 'eval_runtime': 7.2016, 'eval_samples_per_second': 12.081, 'eval_steps_per_second': 0.833, 'epoch': 2.89}
{'loss': 0.0899, 'grad_norm': 0.20583146810531616, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.759436845779419, 'eval_runtime': 7.1467, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.84, 'epoch': 3.11}
{'loss': 0.085, 'grad_norm': 0.3158554434776306, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.7901978492736816, 'eval_runtime': 7.171, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 3.33}
{'loss': 0.086, 'grad_norm': 0.23442919552326202, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.8091328144073486, 'eval_runtime': 7.1423, 'eval_samples_per_second': 12.181, 'eval_steps_per_second': 0.84, 'epoch': 3.56}
{'loss': 0.0849, 'grad_norm': 0.22676825523376465, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.8019824028015137, 'eval_runtime': 7.1781, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.0892, 'grad_norm': 0.20513510704040527, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.760577440261841, 'eval_runtime': 7.1816, 'eval_samples_per_second': 12.114, 'eval_steps_per_second': 0.835, 'epoch': 4.0}
{'loss': 0.0838, 'grad_norm': 0.26302579045295715, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.7633538246154785, 'eval_runtime': 7.1761, 'eval_samples_per_second': 12.124, 'eval_steps_per_second': 0.836, 'epoch': 4.22}
{'loss': 0.0895, 'grad_norm': 0.24908003211021423, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.7796897888183594, 'eval_runtime': 7.1873, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 0.835, 'epoch': 4.44}
{'loss': 0.0937, 'grad_norm': 0.2507018446922302, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.778764009475708, 'eval_runtime': 7.2061, 'eval_samples_per_second': 12.073, 'eval_steps_per_second': 0.833, 'epoch': 4.67}
{'loss': 0.0941, 'grad_norm': 0.3927258253097534, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.776480197906494, 'eval_runtime': 7.2004, 'eval_samples_per_second': 12.083, 'eval_steps_per_second': 0.833, 'epoch': 4.89}
{'train_runtime': 936.1241, 'train_samples_per_second': 3.846, 'train_steps_per_second': 0.24, 'train_loss': 0.08922928293546041, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd0.01...
Fine-tuning for learning_rate=0.001 and weight_decay=0.01 complete. Model saved to './LlamaFinetuned_lr0.001_wd0.01'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=0.005...
{'loss': 0.0603, 'grad_norm': 0.47777697443962097, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.771850347518921, 'eval_runtime': 7.1731, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 0.22}
{'loss': 0.0896, 'grad_norm': 0.3467341661453247, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.768052339553833, 'eval_runtime': 7.192, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 0.44}
{'loss': 0.0911, 'grad_norm': 0.2786461114883423, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.7032716274261475, 'eval_runtime': 7.1749, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 0.67}
{'loss': 0.0933, 'grad_norm': 0.32984018325805664, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.7979936599731445, 'eval_runtime': 7.177, 'eval_samples_per_second': 12.122, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 0.086, 'grad_norm': 0.27533382177352905, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
{'eval_loss': 2.7495696544647217, 'eval_runtime': 7.1704, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 1.11}
{'loss': 0.0844, 'grad_norm': 0.4321666955947876, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.7540388107299805, 'eval_runtime': 7.1766, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.0882, 'grad_norm': 0.3461795449256897, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.8073432445526123, 'eval_runtime': 7.1865, 'eval_samples_per_second': 12.106, 'eval_steps_per_second': 0.835, 'epoch': 1.56}
{'loss': 0.0895, 'grad_norm': 0.3872224986553192, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.775949001312256, 'eval_runtime': 7.1723, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 1.78}
{'loss': 0.0916, 'grad_norm': 0.3767251670360565, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.7412636280059814, 'eval_runtime': 7.1577, 'eval_samples_per_second': 12.155, 'eval_steps_per_second': 0.838, 'epoch': 2.0}
{'loss': 0.081, 'grad_norm': 0.22121982276439667, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.772613763809204, 'eval_runtime': 7.1596, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 0.838, 'epoch': 2.22}
{'loss': 0.0807, 'grad_norm': 0.2689778804779053, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.822643280029297, 'eval_runtime': 7.1797, 'eval_samples_per_second': 12.117, 'eval_steps_per_second': 0.836, 'epoch': 2.44}
{'loss': 0.0822, 'grad_norm': 0.40645432472229004, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.842285394668579, 'eval_runtime': 7.1646, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.0857, 'grad_norm': 0.24530453979969025, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.7789878845214844, 'eval_runtime': 7.1846, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 2.89}
{'loss': 0.0876, 'grad_norm': 0.18626181781291962, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.790639877319336, 'eval_runtime': 7.1449, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 3.11}
{'loss': 0.0832, 'grad_norm': 0.3018558919429779, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.810490846633911, 'eval_runtime': 7.1876, 'eval_samples_per_second': 12.104, 'eval_steps_per_second': 0.835, 'epoch': 3.33}
{'loss': 0.0853, 'grad_norm': 0.24521157145500183, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.81825852394104, 'eval_runtime': 7.1962, 'eval_samples_per_second': 12.09, 'eval_steps_per_second': 0.834, 'epoch': 3.56}
{'loss': 0.0826, 'grad_norm': 0.19888120889663696, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.806065320968628, 'eval_runtime': 7.1704, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.0894, 'grad_norm': 0.46668872237205505, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.7740626335144043, 'eval_runtime': 7.206, 'eval_samples_per_second': 12.073, 'eval_steps_per_second': 0.833, 'epoch': 4.0}
{'loss': 0.0816, 'grad_norm': 0.242911234498024, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.770115375518799, 'eval_runtime': 7.198, 'eval_samples_per_second': 12.087, 'eval_steps_per_second': 0.834, 'epoch': 4.22}
{'loss': 0.0884, 'grad_norm': 0.2731449604034424, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.792630672454834, 'eval_runtime': 7.1943, 'eval_samples_per_second': 12.093, 'eval_steps_per_second': 0.834, 'epoch': 4.44}
{'loss': 0.0924, 'grad_norm': 0.27049124240875244, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.7974162101745605, 'eval_runtime': 7.1944, 'eval_samples_per_second': 12.093, 'eval_steps_per_second': 0.834, 'epoch': 4.67}
{'loss': 0.0932, 'grad_norm': 0.359993577003479, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.793213129043579, 'eval_runtime': 7.1861, 'eval_samples_per_second': 12.107, 'eval_steps_per_second': 0.835, 'epoch': 4.89}
{'train_runtime': 937.589, 'train_samples_per_second': 3.84, 'train_steps_per_second': 0.24, 'train_loss': 0.08606051471498277, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd0.005...
Fine-tuning for learning_rate=0.001 and weight_decay=0.005 complete. Model saved to './LlamaFinetuned_lr0.001_wd0.005'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=0.001...
{'loss': 0.0555, 'grad_norm': 0.3196665346622467, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.888629198074341, 'eval_runtime': 7.2049, 'eval_samples_per_second': 12.075, 'eval_steps_per_second': 0.833, 'epoch': 0.22}
{'loss': 0.0823, 'grad_norm': 0.4191440939903259, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.7502377033233643, 'eval_runtime': 7.1818, 'eval_samples_per_second': 12.114, 'eval_steps_per_second': 0.835, 'epoch': 0.44}
{'loss': 0.0859, 'grad_norm': 0.23473802208900452, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.7420995235443115, 'eval_runtime': 7.1885, 'eval_samples_per_second': 12.103, 'eval_steps_per_second': 0.835, 'epoch': 0.67}
{'loss': 0.0885, 'grad_norm': 0.21893997490406036, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.842719316482544, 'eval_runtime': 7.1977, 'eval_samples_per_second': 12.087, 'eval_steps_per_second': 0.834, 'epoch': 0.89}
{'loss': 0.083, 'grad_norm': 0.2416488379240036, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
{'eval_loss': 2.8529415130615234, 'eval_runtime': 7.1883, 'eval_samples_per_second': 12.103, 'eval_steps_per_second': 0.835, 'epoch': 1.11}
{'loss': 0.0787, 'grad_norm': 0.4002419412136078, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.8138680458068848, 'eval_runtime': 7.1845, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 1.33}
{'loss': 0.0863, 'grad_norm': 0.25128817558288574, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.829261064529419, 'eval_runtime': 7.1927, 'eval_samples_per_second': 12.096, 'eval_steps_per_second': 0.834, 'epoch': 1.56}
{'loss': 0.0819, 'grad_norm': 0.26170969009399414, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.761126756668091, 'eval_runtime': 7.196, 'eval_samples_per_second': 12.09, 'eval_steps_per_second': 0.834, 'epoch': 1.78}
{'loss': 0.0853, 'grad_norm': 0.40208473801612854, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.7664756774902344, 'eval_runtime': 7.2055, 'eval_samples_per_second': 12.074, 'eval_steps_per_second': 0.833, 'epoch': 2.0}
{'loss': 0.077, 'grad_norm': 0.18060824275016785, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.817788600921631, 'eval_runtime': 7.1625, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 2.22}
{'loss': 0.0792, 'grad_norm': 0.2822338938713074, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.858003854751587, 'eval_runtime': 7.174, 'eval_samples_per_second': 12.127, 'eval_steps_per_second': 0.836, 'epoch': 2.44}
{'loss': 0.0793, 'grad_norm': 0.25311553478240967, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.83010196685791, 'eval_runtime': 7.1833, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 2.67}
{'loss': 0.0834, 'grad_norm': 0.28470081090927124, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.8009188175201416, 'eval_runtime': 7.1665, 'eval_samples_per_second': 12.14, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.0848, 'grad_norm': 0.18420672416687012, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.821772336959839, 'eval_runtime': 7.1506, 'eval_samples_per_second': 12.167, 'eval_steps_per_second': 0.839, 'epoch': 3.11}
{'loss': 0.0814, 'grad_norm': 0.32227960228919983, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.8529722690582275, 'eval_runtime': 7.1565, 'eval_samples_per_second': 12.157, 'eval_steps_per_second': 0.838, 'epoch': 3.33}
{'loss': 0.083, 'grad_norm': 0.21459326148033142, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.8539230823516846, 'eval_runtime': 7.1665, 'eval_samples_per_second': 12.14, 'eval_steps_per_second': 0.837, 'epoch': 3.56}
{'loss': 0.0811, 'grad_norm': 0.22472228109836578, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.8420073986053467, 'eval_runtime': 7.1736, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.0863, 'grad_norm': 0.21731221675872803, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.8068690299987793, 'eval_runtime': 7.1586, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 4.0}
{'loss': 0.0805, 'grad_norm': 0.36020681262016296, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.8071889877319336, 'eval_runtime': 7.1716, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 4.22}
{'loss': 0.0874, 'grad_norm': 0.28523653745651245, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.8244080543518066, 'eval_runtime': 7.1674, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.0907, 'grad_norm': 0.2796853184700012, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.826223850250244, 'eval_runtime': 7.1785, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.0933, 'grad_norm': 0.3142559230327606, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.8204991817474365, 'eval_runtime': 7.187, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 0.835, 'epoch': 4.89}
{'train_runtime': 936.384, 'train_samples_per_second': 3.845, 'train_steps_per_second': 0.24, 'train_loss': 0.08280847416983711, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd0.001...
Fine-tuning for learning_rate=0.001 and weight_decay=0.001 complete. Model saved to './LlamaFinetuned_lr0.001_wd0.001'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=0.0005...
{'loss': 0.0496, 'grad_norm': 0.19710783660411835, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.9065065383911133, 'eval_runtime': 7.193, 'eval_samples_per_second': 12.095, 'eval_steps_per_second': 0.834, 'epoch': 0.22}
{'loss': 0.0787, 'grad_norm': 0.39435890316963196, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.7601912021636963, 'eval_runtime': 7.2096, 'eval_samples_per_second': 12.067, 'eval_steps_per_second': 0.832, 'epoch': 0.44}
{'loss': 0.0833, 'grad_norm': 0.3149788975715637, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.7477104663848877, 'eval_runtime': 7.1833, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 0.67}
{'loss': 0.0851, 'grad_norm': 0.24909517168998718, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.8249926567077637, 'eval_runtime': 7.1895, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 0.89}
{'loss': 0.0793, 'grad_norm': 0.5509452819824219, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:54,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:56,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:30<02:39,  3.54s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:36<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:56,  3.98s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:15<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:57,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:31<02:39,  3.55s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:37<00:00,  4.17s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:57,  3.99s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:15<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:30<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:36<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:04<11:57,  3.99s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:15<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:20<05:57,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:31<02:39,  3.54s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:38<00:00,  4.17s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:57,  3.99s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:15<07:57,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:31<02:39,  3.54s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:36<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:55,  3.98s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:31<02:38,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:36<00:00,  4.16s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:54,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 27%|█████████████████████████████████████▌                                                                                                       | 60/225 [04:02<09:40,  3.52s/it]
{'loss': 0.0791, 'grad_norm': 0.35893163084983826, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.776089668273926, 'eval_runtime': 7.1591, 'eval_samples_per_second': 12.152, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
{'loss': 0.0845, 'grad_norm': 0.38608497381210327, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.8201100826263428, 'eval_runtime': 7.1635, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 1.56}
{'loss': 0.0822, 'grad_norm': 0.4279656708240509, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.8037002086639404, 'eval_runtime': 7.1679, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 1.78}
{'loss': 0.0841, 'grad_norm': 0.29515570402145386, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.7939746379852295, 'eval_runtime': 7.153, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 2.0}
{'loss': 0.0772, 'grad_norm': 0.3110012710094452, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.820560932159424, 'eval_runtime': 7.1906, 'eval_samples_per_second': 12.099, 'eval_steps_per_second': 0.834, 'epoch': 2.22}
{'loss': 0.0766, 'grad_norm': 0.23396062850952148, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.8639285564422607, 'eval_runtime': 7.1852, 'eval_samples_per_second': 12.108, 'eval_steps_per_second': 0.835, 'epoch': 2.44}
{'loss': 0.0781, 'grad_norm': 0.24576395750045776, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.8721697330474854, 'eval_runtime': 7.1879, 'eval_samples_per_second': 12.104, 'eval_steps_per_second': 0.835, 'epoch': 2.67}
{'loss': 0.0806, 'grad_norm': 0.26239264011383057, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.8524587154388428, 'eval_runtime': 7.1763, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 2.89}
{'loss': 0.0834, 'grad_norm': 0.24014657735824585, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.8309335708618164, 'eval_runtime': 7.1599, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 0.838, 'epoch': 3.11}
{'loss': 0.0791, 'grad_norm': 0.24398386478424072, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.835024356842041, 'eval_runtime': 7.1687, 'eval_samples_per_second': 12.136, 'eval_steps_per_second': 0.837, 'epoch': 3.33}
{'loss': 0.0817, 'grad_norm': 0.23408615589141846, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.863372564315796, 'eval_runtime': 7.1595, 'eval_samples_per_second': 12.152, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.0793, 'grad_norm': 0.24038256704807281, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.859210729598999, 'eval_runtime': 7.1836, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 3.78}
{'loss': 0.0859, 'grad_norm': 0.3260836899280548, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.8335843086242676, 'eval_runtime': 7.1973, 'eval_samples_per_second': 12.088, 'eval_steps_per_second': 0.834, 'epoch': 4.0}
{'loss': 0.0796, 'grad_norm': 0.23486271500587463, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.826862096786499, 'eval_runtime': 7.1953, 'eval_samples_per_second': 12.091, 'eval_steps_per_second': 0.834, 'epoch': 4.22}
{'loss': 0.0875, 'grad_norm': 0.3029206693172455, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.839153528213501, 'eval_runtime': 7.1837, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 4.44}
{'loss': 0.0902, 'grad_norm': 0.27493831515312195, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.8402864933013916, 'eval_runtime': 7.2016, 'eval_samples_per_second': 12.081, 'eval_steps_per_second': 0.833, 'epoch': 4.67}
{'loss': 0.0923, 'grad_norm': 0.28412312269210815, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.8352160453796387, 'eval_runtime': 7.1876, 'eval_samples_per_second': 12.104, 'eval_steps_per_second': 0.835, 'epoch': 4.89}
{'train_runtime': 938.1362, 'train_samples_per_second': 3.837, 'train_steps_per_second': 0.24, 'train_loss': 0.08115628454420301, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd0.0005...
Fine-tuning for learning_rate=0.001 and weight_decay=0.0005 complete. Model saved to './LlamaFinetuned_lr0.001_wd0.0005'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=0.0001...
{'loss': 0.0464, 'grad_norm': 0.3292483389377594, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.9847209453582764, 'eval_runtime': 7.1827, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 0.22}
{'loss': 0.0732, 'grad_norm': 0.30605390667915344, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.826498031616211, 'eval_runtime': 7.1843, 'eval_samples_per_second': 12.11, 'eval_steps_per_second': 0.835, 'epoch': 0.44}
{'loss': 0.0829, 'grad_norm': 0.39521312713623047, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.694901466369629, 'eval_runtime': 7.2025, 'eval_samples_per_second': 12.079, 'eval_steps_per_second': 0.833, 'epoch': 0.67}
{'loss': 0.0821, 'grad_norm': 0.2983182370662689, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.8028457164764404, 'eval_runtime': 7.2001, 'eval_samples_per_second': 12.083, 'eval_steps_per_second': 0.833, 'epoch': 0.89}
{'loss': 0.0762, 'grad_norm': 0.4515407979488373, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
{'eval_loss': 2.8779757022857666, 'eval_runtime': 7.1735, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 1.11}
{'loss': 0.076, 'grad_norm': 0.3730674386024475, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.835836887359619, 'eval_runtime': 7.2003, 'eval_samples_per_second': 12.083, 'eval_steps_per_second': 0.833, 'epoch': 1.33}
{'loss': 0.0805, 'grad_norm': 0.26923123002052307, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.8815650939941406, 'eval_runtime': 7.1828, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 1.56}
{'loss': 0.0808, 'grad_norm': 0.2570054233074188, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.8113534450531006, 'eval_runtime': 7.1882, 'eval_samples_per_second': 12.103, 'eval_steps_per_second': 0.835, 'epoch': 1.78}
{'loss': 0.0804, 'grad_norm': 0.36382463574409485, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.799370765686035, 'eval_runtime': 7.1688, 'eval_samples_per_second': 12.136, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.0738, 'grad_norm': 0.2548168897628784, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.84965181350708, 'eval_runtime': 7.1822, 'eval_samples_per_second': 12.113, 'eval_steps_per_second': 0.835, 'epoch': 2.22}
{'loss': 0.0738, 'grad_norm': 0.3736322522163391, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.8850724697113037, 'eval_runtime': 7.1877, 'eval_samples_per_second': 12.104, 'eval_steps_per_second': 0.835, 'epoch': 2.44}
{'loss': 0.0763, 'grad_norm': 0.22911988198757172, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.8859331607818604, 'eval_runtime': 7.1849, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 2.67}
{'loss': 0.0771, 'grad_norm': 0.29184556007385254, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.8868701457977295, 'eval_runtime': 7.1786, 'eval_samples_per_second': 12.119, 'eval_steps_per_second': 0.836, 'epoch': 2.89}
{'loss': 0.0807, 'grad_norm': 0.279422402381897, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.881718635559082, 'eval_runtime': 7.1919, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 3.11}
{'loss': 0.0766, 'grad_norm': 0.45223644375801086, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.900569438934326, 'eval_runtime': 7.192, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 3.33}
{'loss': 0.0801, 'grad_norm': 0.25536900758743286, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.9271750450134277, 'eval_runtime': 7.1922, 'eval_samples_per_second': 12.096, 'eval_steps_per_second': 0.834, 'epoch': 3.56}
{'loss': 0.0788, 'grad_norm': 0.23369745910167694, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.899881362915039, 'eval_runtime': 7.1772, 'eval_samples_per_second': 12.122, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.0832, 'grad_norm': 0.22282010316848755, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.8591132164001465, 'eval_runtime': 7.1555, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.839, 'epoch': 4.0}
{'loss': 0.0794, 'grad_norm': 0.24015994369983673, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.854199171066284, 'eval_runtime': 7.1644, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 4.22}
{'loss': 0.0852, 'grad_norm': 0.3004012405872345, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.8663997650146484, 'eval_runtime': 7.1707, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.09, 'grad_norm': 0.30655160546302795, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.865717649459839, 'eval_runtime': 7.1533, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 0.839, 'epoch': 4.67}
{'loss': 0.0933, 'grad_norm': 0.3626856207847595, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.8607847690582275, 'eval_runtime': 7.169, 'eval_samples_per_second': 12.136, 'eval_steps_per_second': 0.837, 'epoch': 4.89}
{'train_runtime': 936.5635, 'train_samples_per_second': 3.844, 'train_steps_per_second': 0.24, 'train_loss': 0.07888734724786546, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd0.0001...
Fine-tuning for learning_rate=0.001 and weight_decay=0.0001 complete. Model saved to './LlamaFinetuned_lr0.001_wd0.0001'.
Starting fine-tuning for learning_rate=0.001 and weight_decay=1e-05...
{'loss': 0.0405, 'grad_norm': 0.33721768856048584, 'learning_rate': 0.0009555555555555556, 'epoch': 0.22}
{'eval_loss': 2.9721262454986572, 'eval_runtime': 7.1905, 'eval_samples_per_second': 12.099, 'eval_steps_per_second': 0.834, 'epoch': 0.22}
{'loss': 0.0704, 'grad_norm': 0.29487892985343933, 'learning_rate': 0.0009111111111111111, 'epoch': 0.44}
{'eval_loss': 2.816025733947754, 'eval_runtime': 7.1827, 'eval_samples_per_second': 12.113, 'eval_steps_per_second': 0.835, 'epoch': 0.44}
{'loss': 0.0761, 'grad_norm': 0.30588608980178833, 'learning_rate': 0.0008666666666666667, 'epoch': 0.67}
{'eval_loss': 2.82029128074646, 'eval_runtime': 7.1832, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 0.67}
{'loss': 0.0752, 'grad_norm': 0.40252360701560974, 'learning_rate': 0.0008222222222222222, 'epoch': 0.89}
{'eval_loss': 2.849478006362915, 'eval_runtime': 7.1749, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 0.0754, 'grad_norm': 0.3191928565502167, 'learning_rate': 0.0007777777777777778, 'epoch': 1.11}
{'eval_loss': 2.845261573791504, 'eval_runtime': 7.1676, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 1.11}
{'loss': 0.0719, 'grad_norm': 0.28369447588920593, 'learning_rate': 0.0007333333333333333, 'epoch': 1.33}
{'eval_loss': 2.8336615562438965, 'eval_runtime': 7.161, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
{'loss': 0.0747, 'grad_norm': 0.48136693239212036, 'learning_rate': 0.000688888888888889, 'epoch': 1.56}
{'eval_loss': 2.9193649291992188, 'eval_runtime': 7.1924, 'eval_samples_per_second': 12.096, 'eval_steps_per_second': 0.834, 'epoch': 1.56}
{'loss': 0.0768, 'grad_norm': 0.35523754358291626, 'learning_rate': 0.0006444444444444444, 'epoch': 1.78}
{'eval_loss': 2.858905076980591, 'eval_runtime': 7.154, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 1.78}
{'loss': 0.0774, 'grad_norm': 0.2948322296142578, 'learning_rate': 0.0006, 'epoch': 2.0}
{'eval_loss': 2.829402446746826, 'eval_runtime': 7.1844, 'eval_samples_per_second': 12.11, 'eval_steps_per_second': 0.835, 'epoch': 2.0}
{'loss': 0.0735, 'grad_norm': 0.35132214426994324, 'learning_rate': 0.0005555555555555556, 'epoch': 2.22}
{'eval_loss': 2.8397014141082764, 'eval_runtime': 7.1609, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 2.22}
{'loss': 0.073, 'grad_norm': 0.25336432456970215, 'learning_rate': 0.0005111111111111111, 'epoch': 2.44}
{'eval_loss': 2.873141288757324, 'eval_runtime': 7.1602, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 2.44}
{'loss': 0.0758, 'grad_norm': 0.40096405148506165, 'learning_rate': 0.00046666666666666666, 'epoch': 2.67}
{'eval_loss': 2.892880916595459, 'eval_runtime': 7.1744, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 2.67}
{'loss': 0.0778, 'grad_norm': 0.25476810336112976, 'learning_rate': 0.0004222222222222222, 'epoch': 2.89}
{'eval_loss': 2.843588352203369, 'eval_runtime': 7.1933, 'eval_samples_per_second': 12.095, 'eval_steps_per_second': 0.834, 'epoch': 2.89}
{'loss': 0.0782, 'grad_norm': 0.2428583800792694, 'learning_rate': 0.00037777777777777777, 'epoch': 3.11}
{'eval_loss': 2.884049892425537, 'eval_runtime': 7.1927, 'eval_samples_per_second': 12.096, 'eval_steps_per_second': 0.834, 'epoch': 3.11}
{'loss': 0.0762, 'grad_norm': 0.39312291145324707, 'learning_rate': 0.0003333333333333333, 'epoch': 3.33}
{'eval_loss': 2.907022714614868, 'eval_runtime': 7.1787, 'eval_samples_per_second': 12.119, 'eval_steps_per_second': 0.836, 'epoch': 3.33}
{'loss': 0.0782, 'grad_norm': 0.2677095830440521, 'learning_rate': 0.0002888888888888889, 'epoch': 3.56}
{'eval_loss': 2.9314334392547607, 'eval_runtime': 7.1405, 'eval_samples_per_second': 12.184, 'eval_steps_per_second': 0.84, 'epoch': 3.56}
{'loss': 0.076, 'grad_norm': 0.24508564174175262, 'learning_rate': 0.00024444444444444443, 'epoch': 3.78}
{'eval_loss': 2.914950132369995, 'eval_runtime': 7.1625, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 3.78}
{'loss': 0.083, 'grad_norm': 0.2586697041988373, 'learning_rate': 0.0002, 'epoch': 4.0}
{'eval_loss': 2.8821356296539307, 'eval_runtime': 7.121, 'eval_samples_per_second': 12.217, 'eval_steps_per_second': 0.843, 'epoch': 4.0}
{'loss': 0.0788, 'grad_norm': 0.23435518145561218, 'learning_rate': 0.00015555555555555556, 'epoch': 4.22}
{'eval_loss': 2.867415428161621, 'eval_runtime': 7.1496, 'eval_samples_per_second': 12.169, 'eval_steps_per_second': 0.839, 'epoch': 4.22}
{'loss': 0.0847, 'grad_norm': 0.3235151767730713, 'learning_rate': 0.0001111111111111111, 'epoch': 4.44}
{'eval_loss': 2.881103277206421, 'eval_runtime': 7.1708, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.0899, 'grad_norm': 0.28810209035873413, 'learning_rate': 6.666666666666667e-05, 'epoch': 4.67}
{'eval_loss': 2.8829731941223145, 'eval_runtime': 7.1494, 'eval_samples_per_second': 12.169, 'eval_steps_per_second': 0.839, 'epoch': 4.67}
{'loss': 0.0922, 'grad_norm': 0.36382243037223816, 'learning_rate': 2.2222222222222223e-05, 'epoch': 4.89}
{'eval_loss': 2.8780810832977295, 'eval_runtime': 7.1433, 'eval_samples_per_second': 12.179, 'eval_steps_per_second': 0.84, 'epoch': 4.89}
{'train_runtime': 936.4082, 'train_samples_per_second': 3.844, 'train_steps_per_second': 0.24, 'train_loss': 0.07662228504816691, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.001_wd1e-05...
Fine-tuning for learning_rate=0.001 and weight_decay=1e-05 complete. Model saved to './LlamaFinetuned_lr0.001_wd1e-05'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=0.1...
{'loss': 0.0361, 'grad_norm': 0.29020610451698303, 'learning_rate': 0.0004777777777777778, 'epoch': 0.22}
{'eval_loss': 3.0797014236450195, 'eval_runtime': 7.1263, 'eval_samples_per_second': 12.208, 'eval_steps_per_second': 0.842, 'epoch': 0.22}
{'loss': 0.0606, 'grad_norm': 0.31609511375427246, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 2.9664933681488037, 'eval_runtime': 7.1657, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 0.44}
{'loss': 0.0666, 'grad_norm': 0.22457829117774963, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 2.8824734687805176, 'eval_runtime': 7.1651, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 0.67}
{'loss': 0.0668, 'grad_norm': 0.16656824946403503, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:13<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:17<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:28<02:38,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:33<00:00,  4.15s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:55,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:56,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:29<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:34<00:00,  4.15s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:03<11:52,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:14<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:17<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:28<02:38,  3.53s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 93%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋         | 210/225 [14:26<00:52,  3.53s/it]
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:06<00:00,  1.12s/it]
 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 5/6 [00:05<00:01,  1.15s/it]
{'loss': 0.0635, 'grad_norm': 0.16290999948978424, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 2.9228413105010986, 'eval_runtime': 7.136, 'eval_samples_per_second': 12.192, 'eval_steps_per_second': 0.841, 'epoch': 1.11}
{'loss': 0.0601, 'grad_norm': 0.26134464144706726, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 2.9598724842071533, 'eval_runtime': 7.1362, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 0.841, 'epoch': 1.33}
{'loss': 0.0635, 'grad_norm': 0.21497218310832977, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 2.9623568058013916, 'eval_runtime': 7.1415, 'eval_samples_per_second': 12.182, 'eval_steps_per_second': 0.84, 'epoch': 1.56}
{'loss': 0.0615, 'grad_norm': 0.9605280160903931, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 2.9473469257354736, 'eval_runtime': 7.1601, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 0.838, 'epoch': 1.78}
{'loss': 0.0654, 'grad_norm': 0.22927217185497284, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 2.9460651874542236, 'eval_runtime': 7.1702, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.0612, 'grad_norm': 0.19101570546627045, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 2.9595108032226562, 'eval_runtime': 7.1693, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.0615, 'grad_norm': 0.17004495859146118, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 2.9881114959716797, 'eval_runtime': 7.1809, 'eval_samples_per_second': 12.116, 'eval_steps_per_second': 0.836, 'epoch': 2.44}
{'loss': 0.0623, 'grad_norm': 0.25859999656677246, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.0236592292785645, 'eval_runtime': 7.1605, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 2.67}
{'loss': 0.0672, 'grad_norm': 0.2705326974391937, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.001110315322876, 'eval_runtime': 7.1512, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 0.839, 'epoch': 2.89}
{'loss': 0.0682, 'grad_norm': 0.3084924519062042, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 2.9953227043151855, 'eval_runtime': 7.165, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 3.11}
{'loss': 0.0667, 'grad_norm': 0.2495107799768448, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 2.9891998767852783, 'eval_runtime': 7.149, 'eval_samples_per_second': 12.169, 'eval_steps_per_second': 0.839, 'epoch': 3.33}
{'loss': 0.0706, 'grad_norm': 0.24005460739135742, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.002185583114624, 'eval_runtime': 7.1317, 'eval_samples_per_second': 12.199, 'eval_steps_per_second': 0.841, 'epoch': 3.56}
{'loss': 0.0689, 'grad_norm': 0.23152974247932434, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 2.9981942176818848, 'eval_runtime': 7.1781, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.0739, 'grad_norm': 0.24759021401405334, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 2.981271743774414, 'eval_runtime': 7.1731, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 4.0}
{'loss': 0.073, 'grad_norm': 0.26007789373397827, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 2.972675085067749, 'eval_runtime': 7.1527, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 4.22}
{'loss': 0.0794, 'grad_norm': 0.2945363223552704, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 2.9732882976531982, 'eval_runtime': 7.1712, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.0864, 'grad_norm': 0.38446664810180664, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
{'eval_loss': 2.9676432609558105, 'eval_runtime': 7.1821, 'eval_samples_per_second': 12.113, 'eval_steps_per_second': 0.835, 'epoch': 4.67}
{'loss': 0.0884, 'grad_norm': 0.38484811782836914, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 2.960649251937866, 'eval_runtime': 7.1691, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 0.837, 'epoch': 4.89}
{'train_runtime': 933.4941, 'train_samples_per_second': 3.856, 'train_steps_per_second': 0.241, 'train_loss': 0.06755973007943895, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd0.1...
Fine-tuning for learning_rate=0.0005 and weight_decay=0.1 complete. Model saved to './LlamaFinetuned_lr0.0005_wd0.1'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=0.05...
{'loss': 0.0296, 'grad_norm': 0.2204594612121582, 'learning_rate': 0.0004777777777777778, 'epoch': 0.22}
{'eval_loss': 3.1085638999938965, 'eval_runtime': 7.1489, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.0562, 'grad_norm': 0.25501564145088196, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 3.0593008995056152, 'eval_runtime': 7.1412, 'eval_samples_per_second': 12.183, 'eval_steps_per_second': 0.84, 'epoch': 0.44}
{'loss': 0.0627, 'grad_norm': 0.20317122340202332, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 2.9890100955963135, 'eval_runtime': 7.1562, 'eval_samples_per_second': 12.157, 'eval_steps_per_second': 0.838, 'epoch': 0.67}
{'loss': 0.0604, 'grad_norm': 0.19516754150390625, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
{'eval_loss': 2.9512953758239746, 'eval_runtime': 7.1836, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 0.89}
{'loss': 0.0602, 'grad_norm': 0.1721072494983673, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 2.9722020626068115, 'eval_runtime': 7.1669, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 1.11}
{'loss': 0.0556, 'grad_norm': 0.19841715693473816, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 3.0050511360168457, 'eval_runtime': 7.1917, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 1.33}
{'loss': 0.0595, 'grad_norm': 0.17614388465881348, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 3.0089478492736816, 'eval_runtime': 7.1781, 'eval_samples_per_second': 12.12, 'eval_steps_per_second': 0.836, 'epoch': 1.56}
{'loss': 0.0599, 'grad_norm': 0.2159527987241745, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 3.0033652782440186, 'eval_runtime': 7.1587, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 1.78}
{'loss': 0.0648, 'grad_norm': 0.4449434280395508, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 2.9596781730651855, 'eval_runtime': 7.1629, 'eval_samples_per_second': 12.146, 'eval_steps_per_second': 0.838, 'epoch': 2.0}
{'loss': 0.0582, 'grad_norm': 0.17357823252677917, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 2.9790122509002686, 'eval_runtime': 7.1884, 'eval_samples_per_second': 12.103, 'eval_steps_per_second': 0.835, 'epoch': 2.22}
{'loss': 0.0598, 'grad_norm': 0.16756851971149445, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 3.0116167068481445, 'eval_runtime': 7.1691, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.0607, 'grad_norm': 0.19665610790252686, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.048044204711914, 'eval_runtime': 7.1709, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.0647, 'grad_norm': 0.21861109137535095, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.0276384353637695, 'eval_runtime': 7.1768, 'eval_samples_per_second': 12.122, 'eval_steps_per_second': 0.836, 'epoch': 2.89}
{'loss': 0.0661, 'grad_norm': 0.18679508566856384, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 3.0324461460113525, 'eval_runtime': 7.1538, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 3.11}
{'loss': 0.0646, 'grad_norm': 0.2447470873594284, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 3.038309097290039, 'eval_runtime': 7.1586, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 3.33}
{'loss': 0.0686, 'grad_norm': 0.23486781120300293, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.053713083267212, 'eval_runtime': 7.1578, 'eval_samples_per_second': 12.155, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.0679, 'grad_norm': 0.2503075897693634, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 3.0381314754486084, 'eval_runtime': 7.1355, 'eval_samples_per_second': 12.193, 'eval_steps_per_second': 0.841, 'epoch': 3.78}
{'loss': 0.0728, 'grad_norm': 0.2507173717021942, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 3.0170488357543945, 'eval_runtime': 7.1651, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 4.0}
{'loss': 0.0719, 'grad_norm': 0.3014342486858368, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 3.009570360183716, 'eval_runtime': 7.1524, 'eval_samples_per_second': 12.164, 'eval_steps_per_second': 0.839, 'epoch': 4.22}
{'loss': 0.079, 'grad_norm': 0.3022680878639221, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 3.0123672485351562, 'eval_runtime': 7.1746, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 4.44}
{'loss': 0.0856, 'grad_norm': 0.3337726593017578, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
{'eval_loss': 3.004894971847534, 'eval_runtime': 7.1734, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.0889, 'grad_norm': 0.40254050493240356, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 2.9960105419158936, 'eval_runtime': 7.1774, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 4.89}
{'train_runtime': 934.721, 'train_samples_per_second': 3.851, 'train_steps_per_second': 0.241, 'train_loss': 0.06512807144059075, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd0.05...
Fine-tuning for learning_rate=0.0005 and weight_decay=0.05 complete. Model saved to './LlamaFinetuned_lr0.0005_wd0.05'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=0.01...
{'loss': 0.0255, 'grad_norm': 0.27855756878852844, 'learning_rate': 0.0004777777777777778, 'epoch': 0.22}
{'eval_loss': 3.166260004043579, 'eval_runtime': 7.1535, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.054, 'grad_norm': 0.24017494916915894, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 3.059863805770874, 'eval_runtime': 7.1398, 'eval_samples_per_second': 12.185, 'eval_steps_per_second': 0.84, 'epoch': 0.44}
{'loss': 0.0573, 'grad_norm': 0.2864314019680023, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 3.0428833961486816, 'eval_runtime': 7.1247, 'eval_samples_per_second': 12.211, 'eval_steps_per_second': 0.842, 'epoch': 0.67}
{'loss': 0.0576, 'grad_norm': 0.21608862280845642, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
{'eval_loss': 3.0342421531677246, 'eval_runtime': 7.1765, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 0.0563, 'grad_norm': 0.22577790915966034, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 3.017771005630493, 'eval_runtime': 7.1343, 'eval_samples_per_second': 12.195, 'eval_steps_per_second': 0.841, 'epoch': 1.11}
{'loss': 0.0544, 'grad_norm': 0.1755014955997467, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 3.0281832218170166, 'eval_runtime': 7.1703, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 1.33}
{'loss': 0.0575, 'grad_norm': 0.18769611418247223, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 3.0427441596984863, 'eval_runtime': 7.1851, 'eval_samples_per_second': 12.108, 'eval_steps_per_second': 0.835, 'epoch': 1.56}
{'loss': 0.0556, 'grad_norm': 0.22768782079219818, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 3.042912244796753, 'eval_runtime': 7.175, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 1.78}
{'loss': 0.0603, 'grad_norm': 0.20874887704849243, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 3.023181676864624, 'eval_runtime': 7.1509, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 0.839, 'epoch': 2.0}
{'loss': 0.056, 'grad_norm': 0.2138386219739914, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 3.033785820007324, 'eval_runtime': 7.1386, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.841, 'epoch': 2.22}
{'loss': 0.0587, 'grad_norm': 0.1961541473865509, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 3.037381172180176, 'eval_runtime': 7.139, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.84, 'epoch': 2.44}
{'loss': 0.0587, 'grad_norm': 0.19839146733283997, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.070847749710083, 'eval_runtime': 7.1529, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 2.67}
{'loss': 0.0631, 'grad_norm': 0.23644806444644928, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.0547356605529785, 'eval_runtime': 7.1491, 'eval_samples_per_second': 12.169, 'eval_steps_per_second': 0.839, 'epoch': 2.89}
{'loss': 0.0645, 'grad_norm': 0.19411051273345947, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 3.059276819229126, 'eval_runtime': 7.1729, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 3.11}
{'loss': 0.0626, 'grad_norm': 0.24202710390090942, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 3.0665507316589355, 'eval_runtime': 7.1651, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 3.33}
{'loss': 0.0666, 'grad_norm': 0.21951445937156677, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.0797595977783203, 'eval_runtime': 7.1512, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 0.839, 'epoch': 3.56}
{'loss': 0.0665, 'grad_norm': 0.2670048177242279, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 3.068343162536621, 'eval_runtime': 7.1795, 'eval_samples_per_second': 12.118, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.0713, 'grad_norm': 0.24319517612457275, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 3.039919376373291, 'eval_runtime': 7.1513, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 0.839, 'epoch': 4.0}
{'loss': 0.0706, 'grad_norm': 0.25809717178344727, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 3.034252405166626, 'eval_runtime': 7.1582, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.0767, 'grad_norm': 0.3232855200767517, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 3.0382323265075684, 'eval_runtime': 7.1582, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 0.838, 'epoch': 4.44}
{'loss': 0.0846, 'grad_norm': 0.3231404721736908, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
  warnings.warn(█████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 5/6 [00:05<00:01,  1.15s/it]
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:02<11:53,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:13<07:55,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:17<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:28<02:38,  3.52s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [15:33<00:00,  4.15s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:02<11:54,  3.97s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 31%|███████████████████████████████████████████▊                                                                                                 | 70/225 [04:43<09:05,  3.52s/it]
 83%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 5/6 [00:05<00:01,  1.15s/it]
  warnings.warn(█████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 5/6 [00:05<00:01,  1.15s/it]
{'loss': 0.0895, 'grad_norm': 0.4335689842700958, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 3.0232856273651123, 'eval_runtime': 7.1644, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 4.89}
{'train_runtime': 934.314, 'train_samples_per_second': 3.853, 'train_steps_per_second': 0.241, 'train_loss': 0.06290583769480387, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd0.01...
Fine-tuning for learning_rate=0.0005 and weight_decay=0.01 complete. Model saved to './LlamaFinetuned_lr0.0005_wd0.01'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=0.005...
{'loss': 0.0244, 'grad_norm': 0.31544923782348633, 'learning_rate': 0.0004777777777777778, 'epoch': 0.22}
{'eval_loss': 3.179896593093872, 'eval_runtime': 7.1367, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 0.841, 'epoch': 0.22}
{'loss': 0.0537, 'grad_norm': 0.326901912689209, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 3.0952720642089844, 'eval_runtime': 7.1478, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 0.839, 'epoch': 0.44}
{'loss': 0.0542, 'grad_norm': 0.20918497443199158, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 3.0538997650146484, 'eval_runtime': 7.1645, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 0.67}
{'loss': 0.0565, 'grad_norm': 0.17617109417915344, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
{'eval_loss': 3.062516450881958, 'eval_runtime': 7.171, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 0.89}
{'loss': 0.0539, 'grad_norm': 0.2588583528995514, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 3.0430476665496826, 'eval_runtime': 7.1591, 'eval_samples_per_second': 12.152, 'eval_steps_per_second': 0.838, 'epoch': 1.11}
{'loss': 0.0505, 'grad_norm': 0.18545234203338623, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 3.060246706008911, 'eval_runtime': 7.1431, 'eval_samples_per_second': 12.18, 'eval_steps_per_second': 0.84, 'epoch': 1.33}
{'loss': 0.0553, 'grad_norm': 0.22122080624103546, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 3.0640757083892822, 'eval_runtime': 7.167, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 1.56}
{'loss': 0.0517, 'grad_norm': 0.2935314476490021, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 3.0648083686828613, 'eval_runtime': 7.1679, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 1.78}
{'loss': 0.0568, 'grad_norm': 0.24420514702796936, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 3.064164400100708, 'eval_runtime': 7.1631, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 2.0}
{'loss': 0.0543, 'grad_norm': 0.19441628456115723, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 3.0741374492645264, 'eval_runtime': 7.171, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.0556, 'grad_norm': 0.2772507667541504, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 3.0769453048706055, 'eval_runtime': 7.1673, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.0567, 'grad_norm': 0.2019662857055664, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.1011037826538086, 'eval_runtime': 7.1652, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.0605, 'grad_norm': 0.24218066036701202, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.0945844650268555, 'eval_runtime': 7.1441, 'eval_samples_per_second': 12.178, 'eval_steps_per_second': 0.84, 'epoch': 2.89}
{'loss': 0.0631, 'grad_norm': 0.2140016257762909, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 3.0947978496551514, 'eval_runtime': 7.1373, 'eval_samples_per_second': 12.19, 'eval_steps_per_second': 0.841, 'epoch': 3.11}
{'loss': 0.0611, 'grad_norm': 0.26359838247299194, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 3.1025726795196533, 'eval_runtime': 7.1418, 'eval_samples_per_second': 12.182, 'eval_steps_per_second': 0.84, 'epoch': 3.33}
{'loss': 0.0652, 'grad_norm': 0.2490948587656021, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.1160078048706055, 'eval_runtime': 7.1376, 'eval_samples_per_second': 12.189, 'eval_steps_per_second': 0.841, 'epoch': 3.56}
{'loss': 0.0656, 'grad_norm': 0.29662594199180603, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 3.095691442489624, 'eval_runtime': 7.1719, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.0698, 'grad_norm': 0.269405335187912, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 3.072578191757202, 'eval_runtime': 7.1705, 'eval_samples_per_second': 12.133, 'eval_steps_per_second': 0.837, 'epoch': 4.0}
{'loss': 0.0688, 'grad_norm': 0.2690912187099457, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 3.0665624141693115, 'eval_runtime': 7.1321, 'eval_samples_per_second': 12.198, 'eval_steps_per_second': 0.841, 'epoch': 4.22}
{'loss': 0.0765, 'grad_norm': 0.3775988817214966, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 3.0637595653533936, 'eval_runtime': 7.119, 'eval_samples_per_second': 12.221, 'eval_steps_per_second': 0.843, 'epoch': 4.44}
{'loss': 0.0847, 'grad_norm': 0.3335161507129669, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
{'eval_loss': 3.0547871589660645, 'eval_runtime': 7.1737, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.0891, 'grad_norm': 0.507806122303009, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 3.0459482669830322, 'eval_runtime': 7.1772, 'eval_samples_per_second': 12.122, 'eval_steps_per_second': 0.836, 'epoch': 4.89}
{'train_runtime': 934.0247, 'train_samples_per_second': 3.854, 'train_steps_per_second': 0.241, 'train_loss': 0.061200569404496086, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd0.005...
Fine-tuning for learning_rate=0.0005 and weight_decay=0.005 complete. Model saved to './LlamaFinetuned_lr0.0005_wd0.005'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=0.001...
{'loss': 0.0211, 'grad_norm': 0.3016197979450226, 'learning_rate': 0.0004777777777777778, 'epoch': 0.22}
{'eval_loss': 3.2011992931365967, 'eval_runtime': 7.1448, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 0.22}
{'loss': 0.0455, 'grad_norm': 0.2553896903991699, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 3.1461212635040283, 'eval_runtime': 7.1723, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 0.44}
{'loss': 0.0504, 'grad_norm': 0.2709825038909912, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 3.1021976470947266, 'eval_runtime': 7.1657, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 0.67}
{'loss': 0.0532, 'grad_norm': 0.29369720816612244, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
{'eval_loss': 3.078991413116455, 'eval_runtime': 7.1393, 'eval_samples_per_second': 12.186, 'eval_steps_per_second': 0.84, 'epoch': 0.89}
{'loss': 0.0497, 'grad_norm': 0.25104719400405884, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 3.102155923843384, 'eval_runtime': 7.1558, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.838, 'epoch': 1.11}
{'loss': 0.0485, 'grad_norm': 0.1769946962594986, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 3.084085464477539, 'eval_runtime': 7.1531, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 1.33}
{'loss': 0.0517, 'grad_norm': 0.22008390724658966, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 3.0806877613067627, 'eval_runtime': 7.138, 'eval_samples_per_second': 12.188, 'eval_steps_per_second': 0.841, 'epoch': 1.56}
{'loss': 0.0485, 'grad_norm': 0.2953217923641205, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 3.085587739944458, 'eval_runtime': 7.1575, 'eval_samples_per_second': 12.155, 'eval_steps_per_second': 0.838, 'epoch': 1.78}
{'loss': 0.0546, 'grad_norm': 0.25337210297584534, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 3.059676170349121, 'eval_runtime': 7.1752, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 2.0}
{'loss': 0.0506, 'grad_norm': 0.2136707752943039, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 3.0889909267425537, 'eval_runtime': 7.1518, 'eval_samples_per_second': 12.165, 'eval_steps_per_second': 0.839, 'epoch': 2.22}
{'loss': 0.0526, 'grad_norm': 0.22414030134677887, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 3.1017472743988037, 'eval_runtime': 7.1678, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.0541, 'grad_norm': 0.17463995516300201, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.121380567550659, 'eval_runtime': 7.1345, 'eval_samples_per_second': 12.194, 'eval_steps_per_second': 0.841, 'epoch': 2.67}
{'loss': 0.0588, 'grad_norm': 0.26606234908103943, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.105520725250244, 'eval_runtime': 7.1595, 'eval_samples_per_second': 12.152, 'eval_steps_per_second': 0.838, 'epoch': 2.89}
{'loss': 0.0601, 'grad_norm': 0.23691076040267944, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 3.105170965194702, 'eval_runtime': 7.1471, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.84, 'epoch': 3.11}
{'loss': 0.0589, 'grad_norm': 0.2715143859386444, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 3.1141936779022217, 'eval_runtime': 7.1496, 'eval_samples_per_second': 12.169, 'eval_steps_per_second': 0.839, 'epoch': 3.33}
{'loss': 0.0628, 'grad_norm': 0.27301034331321716, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.1228458881378174, 'eval_runtime': 7.1527, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 3.56}
{'loss': 0.0642, 'grad_norm': 0.30084413290023804, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 3.102940320968628, 'eval_runtime': 7.1429, 'eval_samples_per_second': 12.18, 'eval_steps_per_second': 0.84, 'epoch': 3.78}
{'loss': 0.0681, 'grad_norm': 0.26313844323158264, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 3.0833680629730225, 'eval_runtime': 7.1545, 'eval_samples_per_second': 12.16, 'eval_steps_per_second': 0.839, 'epoch': 4.0}
{'loss': 0.0687, 'grad_norm': 0.30812907218933105, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 3.079345941543579, 'eval_runtime': 7.1276, 'eval_samples_per_second': 12.206, 'eval_steps_per_second': 0.842, 'epoch': 4.22}
{'loss': 0.076, 'grad_norm': 0.3367481529712677, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 3.078348398208618, 'eval_runtime': 7.1415, 'eval_samples_per_second': 12.182, 'eval_steps_per_second': 0.84, 'epoch': 4.44}
{'loss': 0.0847, 'grad_norm': 0.3553522229194641, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
{'eval_loss': 3.066622018814087, 'eval_runtime': 7.1724, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 4.67}
{'loss': 0.0894, 'grad_norm': 0.4403543770313263, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 3.0573575496673584, 'eval_runtime': 7.1587, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 4.89}
{'train_runtime': 934.0005, 'train_samples_per_second': 3.854, 'train_steps_per_second': 0.241, 'train_loss': 0.05876539170742035, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd0.001...
Fine-tuning for learning_rate=0.0005 and weight_decay=0.001 complete. Model saved to './LlamaFinetuned_lr0.0005_wd0.001'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=0.0005...
{'loss': 0.0176, 'grad_norm': 0.2885974049568176, 'learning_rate': 0.0004777777777777778, 'epoch': 0.22}
{'eval_loss': 3.2707526683807373, 'eval_runtime': 7.1594, 'eval_samples_per_second': 12.152, 'eval_steps_per_second': 0.838, 'epoch': 0.22}
{'loss': 0.0422, 'grad_norm': 0.29171040654182434, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 3.155907392501831, 'eval_runtime': 7.1607, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 0.44}
{'loss': 0.0474, 'grad_norm': 0.262482613325119, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 3.128859281539917, 'eval_runtime': 7.1487, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 0.839, 'epoch': 0.67}
{'loss': 0.0495, 'grad_norm': 0.2256159484386444, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
{'eval_loss': 3.1314282417297363, 'eval_runtime': 7.1335, 'eval_samples_per_second': 12.196, 'eval_steps_per_second': 0.841, 'epoch': 0.89}
{'loss': 0.0489, 'grad_norm': 0.2870630621910095, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 3.122992753982544, 'eval_runtime': 7.1367, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 0.841, 'epoch': 1.11}
{'loss': 0.0466, 'grad_norm': 0.212191641330719, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 3.0940632820129395, 'eval_runtime': 7.1535, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 0.839, 'epoch': 1.33}
{'loss': 0.0501, 'grad_norm': 0.2649880051612854, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 3.1115565299987793, 'eval_runtime': 7.1576, 'eval_samples_per_second': 12.155, 'eval_steps_per_second': 0.838, 'epoch': 1.56}
{'loss': 0.047, 'grad_norm': 0.2579871416091919, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 3.1176564693450928, 'eval_runtime': 7.139, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.84, 'epoch': 1.78}
{'loss': 0.0504, 'grad_norm': 0.23921555280685425, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 3.106783866882324, 'eval_runtime': 7.1441, 'eval_samples_per_second': 12.178, 'eval_steps_per_second': 0.84, 'epoch': 2.0}
{'loss': 0.0488, 'grad_norm': 0.2732268273830414, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 3.1240546703338623, 'eval_runtime': 7.1415, 'eval_samples_per_second': 12.182, 'eval_steps_per_second': 0.84, 'epoch': 2.22}
{'loss': 0.0509, 'grad_norm': 0.21470284461975098, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 3.1394436359405518, 'eval_runtime': 7.1525, 'eval_samples_per_second': 12.164, 'eval_steps_per_second': 0.839, 'epoch': 2.44}
{'loss': 0.0511, 'grad_norm': 0.20107945799827576, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.167699098587036, 'eval_runtime': 7.1267, 'eval_samples_per_second': 12.208, 'eval_steps_per_second': 0.842, 'epoch': 2.67}
{'loss': 0.0561, 'grad_norm': 0.27574464678764343, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.156468152999878, 'eval_runtime': 7.1794, 'eval_samples_per_second': 12.118, 'eval_steps_per_second': 0.836, 'epoch': 2.89}
{'loss': 0.0574, 'grad_norm': 0.25157833099365234, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 3.144620656967163, 'eval_runtime': 7.1449, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 3.11}
{'loss': 0.0569, 'grad_norm': 0.35897883772850037, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 3.148442029953003, 'eval_runtime': 7.1638, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 3.33}
{'loss': 0.0604, 'grad_norm': 0.2732800245285034, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.156252861022949, 'eval_runtime': 7.1678, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 3.56}
{'loss': 0.0622, 'grad_norm': 0.2700791656970978, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 3.1394429206848145, 'eval_runtime': 7.165, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.0667, 'grad_norm': 0.30054038763046265, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 3.1213760375976562, 'eval_runtime': 7.1912, 'eval_samples_per_second': 12.098, 'eval_steps_per_second': 0.834, 'epoch': 4.0}
{'loss': 0.068, 'grad_norm': 0.3186066746711731, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 3.1147968769073486, 'eval_runtime': 7.1448, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 4.22}
{'loss': 0.0748, 'grad_norm': 0.34704798460006714, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 3.110689401626587, 'eval_runtime': 7.1688, 'eval_samples_per_second': 12.136, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.085, 'grad_norm': 0.3365970253944397, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
{'eval_loss': 3.098062038421631, 'eval_runtime': 7.1444, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 4.67}
{'loss': 0.0902, 'grad_norm': 0.5150410532951355, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 3.087970495223999, 'eval_runtime': 7.1874, 'eval_samples_per_second': 12.104, 'eval_steps_per_second': 0.835, 'epoch': 4.89}
{'train_runtime': 934.1229, 'train_samples_per_second': 3.854, 'train_steps_per_second': 0.241, 'train_loss': 0.05685196691089206, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd0.0005...
Fine-tuning for learning_rate=0.0005 and weight_decay=0.0005 complete. Model saved to './LlamaFinetuned_lr0.0005_wd0.0005'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=0.0001...
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
  warnings.warn(
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:309: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
  0%|                                                                                                                                                      | 0/225 [00:00<?, ?it/s]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 20%|████████████████████████████▏                                                                                                                | 45/225 [03:02<11:53,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:13<07:54,  3.51s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
 45%|██████████████████████████████████████████████████████████████▊                                                                             | 101/225 [06:59<11:40,  5.65s/it]
/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead
{'eval_loss': 3.2618775367736816, 'eval_runtime': 7.1292, 'eval_samples_per_second': 12.203, 'eval_steps_per_second': 0.842, 'epoch': 0.22}
{'loss': 0.0453, 'grad_norm': 0.36158719658851624, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 3.1951472759246826, 'eval_runtime': 7.1542, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 0.44}
{'loss': 0.0478, 'grad_norm': 0.23323190212249756, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 3.1270320415496826, 'eval_runtime': 7.1449, 'eval_samples_per_second': 12.176, 'eval_steps_per_second': 0.84, 'epoch': 0.67}
{'loss': 0.0478, 'grad_norm': 0.1852785050868988, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
{'eval_loss': 3.162768602371216, 'eval_runtime': 7.1652, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 0.89}
{'loss': 0.0469, 'grad_norm': 0.21496008336544037, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 3.20343017578125, 'eval_runtime': 7.1557, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.838, 'epoch': 1.11}
{'loss': 0.0433, 'grad_norm': 0.2171981930732727, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 3.214073419570923, 'eval_runtime': 7.1417, 'eval_samples_per_second': 12.182, 'eval_steps_per_second': 0.84, 'epoch': 1.33}
{'loss': 0.0474, 'grad_norm': 0.2671743631362915, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 3.170609474182129, 'eval_runtime': 7.1665, 'eval_samples_per_second': 12.14, 'eval_steps_per_second': 0.837, 'epoch': 1.56}
{'loss': 0.0431, 'grad_norm': 0.19136857986450195, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 3.151761054992676, 'eval_runtime': 7.187, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 0.835, 'epoch': 1.78}
{'loss': 0.049, 'grad_norm': 0.2808679938316345, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 3.116382360458374, 'eval_runtime': 7.1565, 'eval_samples_per_second': 12.157, 'eval_steps_per_second': 0.838, 'epoch': 2.0}
{'loss': 0.0475, 'grad_norm': 0.3222699761390686, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 3.1162052154541016, 'eval_runtime': 7.1449, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 2.22}
{'loss': 0.0494, 'grad_norm': 0.2571580708026886, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 3.1223130226135254, 'eval_runtime': 7.1482, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.839, 'epoch': 2.44}
{'loss': 0.0511, 'grad_norm': 0.1647741049528122, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.1452062129974365, 'eval_runtime': 7.1603, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 2.67}
{'loss': 0.0543, 'grad_norm': 0.24829527735710144, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.149038076400757, 'eval_runtime': 7.1472, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.839, 'epoch': 2.89}
{'loss': 0.0551, 'grad_norm': 0.26011136174201965, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 3.1698997020721436, 'eval_runtime': 7.1309, 'eval_samples_per_second': 12.2, 'eval_steps_per_second': 0.841, 'epoch': 3.11}
{'loss': 0.0554, 'grad_norm': 0.3389381766319275, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 3.171102523803711, 'eval_runtime': 7.1642, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.837, 'epoch': 3.33}
{'loss': 0.0583, 'grad_norm': 0.29286178946495056, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.1730101108551025, 'eval_runtime': 7.1342, 'eval_samples_per_second': 12.195, 'eval_steps_per_second': 0.841, 'epoch': 3.56}
{'loss': 0.0604, 'grad_norm': 0.25406578183174133, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 3.153501033782959, 'eval_runtime': 7.1484, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 0.839, 'epoch': 3.78}
{'loss': 0.0643, 'grad_norm': 0.2888597249984741, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 3.1373445987701416, 'eval_runtime': 7.1291, 'eval_samples_per_second': 12.204, 'eval_steps_per_second': 0.842, 'epoch': 4.0}
{'loss': 0.0675, 'grad_norm': 0.29944148659706116, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 3.128862142562866, 'eval_runtime': 7.1791, 'eval_samples_per_second': 12.119, 'eval_steps_per_second': 0.836, 'epoch': 4.22}
{'loss': 0.0739, 'grad_norm': 0.36964109539985657, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 3.1250839233398438, 'eval_runtime': 7.136, 'eval_samples_per_second': 12.192, 'eval_steps_per_second': 0.841, 'epoch': 4.44}
{'loss': 0.0842, 'grad_norm': 0.35114747285842896, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
{'eval_loss': 3.114398241043091, 'eval_runtime': 7.1215, 'eval_samples_per_second': 12.217, 'eval_steps_per_second': 0.843, 'epoch': 4.67}
{'loss': 0.0903, 'grad_norm': 0.4572507441043854, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 3.1049797534942627, 'eval_runtime': 7.1364, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 0.841, 'epoch': 4.89}
{'train_runtime': 934.5804, 'train_samples_per_second': 3.852, 'train_steps_per_second': 0.241, 'train_loss': 0.05561158643828498, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd0.0001...
Fine-tuning for learning_rate=0.0005 and weight_decay=0.0001 complete. Model saved to './LlamaFinetuned_lr0.0005_wd0.0001'.
Starting fine-tuning for learning_rate=0.0005 and weight_decay=1e-05...
{'loss': 0.012, 'grad_norm': 0.1997053176164627, 'learning_rate': 0.0004777777777777778, 'epoch': 0.22}
{'eval_loss': 3.27767276763916, 'eval_runtime': 7.1073, 'eval_samples_per_second': 12.241, 'eval_steps_per_second': 0.844, 'epoch': 0.22}
{'loss': 0.0376, 'grad_norm': 0.6375426054000854, 'learning_rate': 0.00045555555555555556, 'epoch': 0.44}
{'eval_loss': 3.2358717918395996, 'eval_runtime': 7.1325, 'eval_samples_per_second': 12.198, 'eval_steps_per_second': 0.841, 'epoch': 0.44}
{'loss': 0.0465, 'grad_norm': 0.29709625244140625, 'learning_rate': 0.00043333333333333337, 'epoch': 0.67}
{'eval_loss': 3.167159080505371, 'eval_runtime': 7.1465, 'eval_samples_per_second': 12.174, 'eval_steps_per_second': 0.84, 'epoch': 0.67}
{'loss': 0.0478, 'grad_norm': 0.21441026031970978, 'learning_rate': 0.0004111111111111111, 'epoch': 0.89}
{'eval_loss': 3.160473108291626, 'eval_runtime': 7.1513, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 0.839, 'epoch': 0.89}
{'loss': 0.0442, 'grad_norm': 0.24738579988479614, 'learning_rate': 0.0003888888888888889, 'epoch': 1.11}
{'eval_loss': 3.173936128616333, 'eval_runtime': 7.1437, 'eval_samples_per_second': 12.179, 'eval_steps_per_second': 0.84, 'epoch': 1.11}
{'loss': 0.0411, 'grad_norm': 0.23512257635593414, 'learning_rate': 0.00036666666666666667, 'epoch': 1.33}
{'eval_loss': 3.2131237983703613, 'eval_runtime': 7.1692, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 0.837, 'epoch': 1.33}
{'loss': 0.0439, 'grad_norm': 0.2633407413959503, 'learning_rate': 0.0003444444444444445, 'epoch': 1.56}
{'eval_loss': 3.2060556411743164, 'eval_runtime': 7.1744, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 1.56}
{'loss': 0.043, 'grad_norm': 0.2825017273426056, 'learning_rate': 0.0003222222222222222, 'epoch': 1.78}
{'eval_loss': 3.165544271469116, 'eval_runtime': 7.1661, 'eval_samples_per_second': 12.14, 'eval_steps_per_second': 0.837, 'epoch': 1.78}
{'loss': 0.047, 'grad_norm': 0.25480732321739197, 'learning_rate': 0.0003, 'epoch': 2.0}
{'eval_loss': 3.107764482498169, 'eval_runtime': 7.1528, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 2.0}
{'loss': 0.0442, 'grad_norm': 0.23930905759334564, 'learning_rate': 0.0002777777777777778, 'epoch': 2.22}
{'eval_loss': 3.1049976348876953, 'eval_runtime': 7.1682, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.0467, 'grad_norm': 0.3063889741897583, 'learning_rate': 0.00025555555555555553, 'epoch': 2.44}
{'eval_loss': 3.1474061012268066, 'eval_runtime': 7.1386, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.841, 'epoch': 2.44}
{'loss': 0.0482, 'grad_norm': 0.2090553492307663, 'learning_rate': 0.00023333333333333333, 'epoch': 2.67}
{'eval_loss': 3.1875619888305664, 'eval_runtime': 7.169, 'eval_samples_per_second': 12.136, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.0547, 'grad_norm': 0.25288698077201843, 'learning_rate': 0.0002111111111111111, 'epoch': 2.89}
{'eval_loss': 3.164811372756958, 'eval_runtime': 7.13, 'eval_samples_per_second': 12.202, 'eval_steps_per_second': 0.842, 'epoch': 2.89}
{'loss': 0.0533, 'grad_norm': 0.28896695375442505, 'learning_rate': 0.00018888888888888888, 'epoch': 3.11}
{'eval_loss': 3.172914743423462, 'eval_runtime': 7.1521, 'eval_samples_per_second': 12.164, 'eval_steps_per_second': 0.839, 'epoch': 3.11}
{'loss': 0.0538, 'grad_norm': 0.30202409625053406, 'learning_rate': 0.00016666666666666666, 'epoch': 3.33}
{'eval_loss': 3.1692683696746826, 'eval_runtime': 7.155, 'eval_samples_per_second': 12.159, 'eval_steps_per_second': 0.839, 'epoch': 3.33}
{'loss': 0.0567, 'grad_norm': 0.303052693605423, 'learning_rate': 0.00014444444444444444, 'epoch': 3.56}
{'eval_loss': 3.180539131164551, 'eval_runtime': 7.1583, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.0599, 'grad_norm': 0.39367806911468506, 'learning_rate': 0.00012222222222222221, 'epoch': 3.78}
{'eval_loss': 3.161342144012451, 'eval_runtime': 7.1643, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.0637, 'grad_norm': 0.32055678963661194, 'learning_rate': 0.0001, 'epoch': 4.0}
{'eval_loss': 3.1441826820373535, 'eval_runtime': 7.1788, 'eval_samples_per_second': 12.119, 'eval_steps_per_second': 0.836, 'epoch': 4.0}
{'loss': 0.0664, 'grad_norm': 0.3277618885040283, 'learning_rate': 7.777777777777778e-05, 'epoch': 4.22}
{'eval_loss': 3.1418700218200684, 'eval_runtime': 7.1645, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 4.22}
{'loss': 0.0732, 'grad_norm': 0.3709169030189514, 'learning_rate': 5.555555555555555e-05, 'epoch': 4.44}
{'eval_loss': 3.134895086288452, 'eval_runtime': 7.1543, 'eval_samples_per_second': 12.16, 'eval_steps_per_second': 0.839, 'epoch': 4.44}
{'loss': 0.085, 'grad_norm': 0.3935384750366211, 'learning_rate': 3.3333333333333335e-05, 'epoch': 4.67}
{'eval_loss': 3.1211442947387695, 'eval_runtime': 7.1618, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 4.67}
{'loss': 0.0915, 'grad_norm': 0.49044081568717957, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.89}
{'eval_loss': 3.112239122390747, 'eval_runtime': 7.174, 'eval_samples_per_second': 12.127, 'eval_steps_per_second': 0.836, 'epoch': 4.89}
{'train_runtime': 934.4559, 'train_samples_per_second': 3.853, 'train_steps_per_second': 0.241, 'train_loss': 0.053903552724255455, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0005_wd1e-05...
Fine-tuning for learning_rate=0.0005 and weight_decay=1e-05 complete. Model saved to './LlamaFinetuned_lr0.0005_wd1e-05'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=0.1...
{'loss': 0.0139, 'grad_norm': 0.26107552647590637, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.1952714920043945, 'eval_runtime': 7.1235, 'eval_samples_per_second': 12.213, 'eval_steps_per_second': 0.842, 'epoch': 0.22}
{'loss': 0.0342, 'grad_norm': 0.3406940698623657, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.223168134689331, 'eval_runtime': 7.1369, 'eval_samples_per_second': 12.19, 'eval_steps_per_second': 0.841, 'epoch': 0.44}
{'loss': 0.0371, 'grad_norm': 0.3112126886844635, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.2250447273254395, 'eval_runtime': 7.1382, 'eval_samples_per_second': 12.188, 'eval_steps_per_second': 0.841, 'epoch': 0.67}
{'loss': 0.0364, 'grad_norm': 0.1719272881746292, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.2100696563720703, 'eval_runtime': 7.1451, 'eval_samples_per_second': 12.176, 'eval_steps_per_second': 0.84, 'epoch': 0.89}
{'loss': 0.0364, 'grad_norm': 0.23130875825881958, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.2123069763183594, 'eval_runtime': 7.1497, 'eval_samples_per_second': 12.168, 'eval_steps_per_second': 0.839, 'epoch': 1.11}
{'loss': 0.0341, 'grad_norm': 0.20134113729000092, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.2319605350494385, 'eval_runtime': 7.1729, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.0349, 'grad_norm': 0.21193350851535797, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.56}
{'eval_loss': 3.2369184494018555, 'eval_runtime': 7.1478, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 0.839, 'epoch': 1.56}
{'loss': 0.0348, 'grad_norm': 0.280682235956192, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}
{'eval_loss': 3.2404487133026123, 'eval_runtime': 7.17, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 0.837, 'epoch': 1.78}
{'loss': 0.0383, 'grad_norm': 0.3242115080356598, 'learning_rate': 6e-05, 'epoch': 2.0}
{'eval_loss': 3.249002695083618, 'eval_runtime': 7.1659, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.038, 'grad_norm': 0.24964579939842224, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}
{'eval_loss': 3.221963882446289, 'eval_runtime': 7.1408, 'eval_samples_per_second': 12.184, 'eval_steps_per_second': 0.84, 'epoch': 2.22}
{'loss': 0.0386, 'grad_norm': 0.25704821944236755, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.44}
{'eval_loss': 3.2142245769500732, 'eval_runtime': 7.1235, 'eval_samples_per_second': 12.213, 'eval_steps_per_second': 0.842, 'epoch': 2.44}
{'loss': 0.0414, 'grad_norm': 0.26085883378982544, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}
{'eval_loss': 3.216973066329956, 'eval_runtime': 7.1447, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 2.67}
{'loss': 0.046, 'grad_norm': 0.4050215780735016, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.89}
{'eval_loss': 3.2145042419433594, 'eval_runtime': 7.1376, 'eval_samples_per_second': 12.189, 'eval_steps_per_second': 0.841, 'epoch': 2.89}
{'loss': 0.0465, 'grad_norm': 0.2901868224143982, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}
{'eval_loss': 3.2143211364746094, 'eval_runtime': 7.1633, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 3.11}
{'loss': 0.0469, 'grad_norm': 0.25658857822418213, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}
{'eval_loss': 3.218343496322632, 'eval_runtime': 7.1397, 'eval_samples_per_second': 12.185, 'eval_steps_per_second': 0.84, 'epoch': 3.33}
{'loss': 0.0514, 'grad_norm': 0.29129186272621155, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.21994686126709, 'eval_runtime': 7.1563, 'eval_samples_per_second': 12.157, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.0536, 'grad_norm': 0.2911244332790375, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.2169101238250732, 'eval_runtime': 7.164, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 3.78}
 49%|████████████████████████████████████████████████████████████████████▍                                                                       | 110/225 [07:30<06:45,  3.53s/it].46 of 🤗 Transformers. Use `eval_strategy` instead
 49%|████████████████████████████████████████████████████████████████████▍                                                                       | 110/225 [07:30<06:45,  3.53s/it].46 of 🤗 Transformers. Use `eval_strategy` instead
{'eval_loss': 3.208345890045166, 'eval_runtime': 7.147, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.84, 'epoch': 4.0}
{'loss': 0.0625, 'grad_norm': 0.38928481936454773, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.2032103538513184, 'eval_runtime': 7.1535, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 0.839, 'epoch': 4.22}
{'loss': 0.0712, 'grad_norm': 0.5037790536880493, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
{'eval_loss': 3.199826717376709, 'eval_runtime': 7.1654, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 4.44}
{'loss': 0.0818, 'grad_norm': 0.41276296973228455, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.194342613220215, 'eval_runtime': 7.155, 'eval_samples_per_second': 12.159, 'eval_steps_per_second': 0.839, 'epoch': 4.67}
{'loss': 0.0902, 'grad_norm': 0.5801137089729309, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.190657377243042, 'eval_runtime': 7.1305, 'eval_samples_per_second': 12.201, 'eval_steps_per_second': 0.841, 'epoch': 4.89}
{'train_runtime': 933.4784, 'train_samples_per_second': 3.857, 'train_steps_per_second': 0.241, 'train_loss': 0.04793080270290375, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd0.1...
Fine-tuning for learning_rate=0.0001 and weight_decay=0.1 complete. Model saved to './LlamaFinetuned_lr0.0001_wd0.1'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=0.05...
{'loss': 0.0083, 'grad_norm': 0.2568220794200897, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.263805627822876, 'eval_runtime': 7.165, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 0.22}
{'loss': 0.0297, 'grad_norm': 0.45537224411964417, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.270601749420166, 'eval_runtime': 7.1385, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.841, 'epoch': 0.44}
{'loss': 0.0329, 'grad_norm': 0.40292873978614807, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.2682807445526123, 'eval_runtime': 7.1488, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 0.839, 'epoch': 0.67}
{'loss': 0.0337, 'grad_norm': 0.19714345037937164, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.24375057220459, 'eval_runtime': 7.167, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 0.89}
{'loss': 0.0335, 'grad_norm': 0.237122043967247, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.237898111343384, 'eval_runtime': 7.1656, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 1.11}
{'loss': 0.0317, 'grad_norm': 0.20182543992996216, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.2532012462615967, 'eval_runtime': 7.1722, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 1.33}
{'loss': 0.0324, 'grad_norm': 0.2118905782699585, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.56}
{'eval_loss': 3.2526116371154785, 'eval_runtime': 7.1392, 'eval_samples_per_second': 12.186, 'eval_steps_per_second': 0.84, 'epoch': 1.56}
{'loss': 0.0323, 'grad_norm': 0.3005124032497406, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}
{'eval_loss': 3.2567710876464844, 'eval_runtime': 7.1421, 'eval_samples_per_second': 12.181, 'eval_steps_per_second': 0.84, 'epoch': 1.78}
{'loss': 0.0357, 'grad_norm': 0.2854674756526947, 'learning_rate': 6e-05, 'epoch': 2.0}
{'eval_loss': 3.265505790710449, 'eval_runtime': 7.1684, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.0359, 'grad_norm': 0.27397170662879944, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}
{'eval_loss': 3.2438018321990967, 'eval_runtime': 7.1872, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 0.835, 'epoch': 2.22}
{'loss': 0.0367, 'grad_norm': 0.32169947028160095, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.44}
{'eval_loss': 3.2356743812561035, 'eval_runtime': 7.1858, 'eval_samples_per_second': 12.107, 'eval_steps_per_second': 0.835, 'epoch': 2.44}
{'loss': 0.0397, 'grad_norm': 0.31326475739479065, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}
{'eval_loss': 3.2380220890045166, 'eval_runtime': 7.1812, 'eval_samples_per_second': 12.115, 'eval_steps_per_second': 0.836, 'epoch': 2.67}
{'loss': 0.0451, 'grad_norm': 0.4747861623764038, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.89}
{'eval_loss': 3.2331044673919678, 'eval_runtime': 7.1727, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.0451, 'grad_norm': 0.3201952576637268, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}
{'eval_loss': 3.232671022415161, 'eval_runtime': 7.1329, 'eval_samples_per_second': 12.197, 'eval_steps_per_second': 0.841, 'epoch': 3.11}
{'loss': 0.046, 'grad_norm': 0.26786473393440247, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}
{'eval_loss': 3.2351207733154297, 'eval_runtime': 7.1338, 'eval_samples_per_second': 12.195, 'eval_steps_per_second': 0.841, 'epoch': 3.33}
{'loss': 0.0507, 'grad_norm': 0.3059728443622589, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.2358593940734863, 'eval_runtime': 7.1385, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.841, 'epoch': 3.56}
{'loss': 0.0534, 'grad_norm': 0.2882246971130371, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.231494665145874, 'eval_runtime': 7.117, 'eval_samples_per_second': 12.224, 'eval_steps_per_second': 0.843, 'epoch': 3.78}
{'loss': 0.0574, 'grad_norm': 0.3138992190361023, 'learning_rate': 2e-05, 'epoch': 4.0}
{'eval_loss': 3.222125291824341, 'eval_runtime': 7.1325, 'eval_samples_per_second': 12.198, 'eval_steps_per_second': 0.841, 'epoch': 4.0}
{'loss': 0.0628, 'grad_norm': 0.43090683221817017, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.2163991928100586, 'eval_runtime': 7.147, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.84, 'epoch': 4.22}
{'loss': 0.0716, 'grad_norm': 0.5401710271835327, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
{'eval_loss': 3.2129814624786377, 'eval_runtime': 7.1229, 'eval_samples_per_second': 12.214, 'eval_steps_per_second': 0.842, 'epoch': 4.44}
{'loss': 0.0828, 'grad_norm': 0.44298064708709717, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.207139015197754, 'eval_runtime': 7.145, 'eval_samples_per_second': 12.176, 'eval_steps_per_second': 0.84, 'epoch': 4.67}
{'loss': 0.0918, 'grad_norm': 0.6131369471549988, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.2034196853637695, 'eval_runtime': 7.1489, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 0.839, 'epoch': 4.89}
{'train_runtime': 933.3452, 'train_samples_per_second': 3.857, 'train_steps_per_second': 0.241, 'train_loss': 0.04636355214648777, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd0.05...
Fine-tuning for learning_rate=0.0001 and weight_decay=0.05 complete. Model saved to './LlamaFinetuned_lr0.0001_wd0.05'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=0.01...
{'loss': 0.0066, 'grad_norm': 0.21479137241840363, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.278986692428589, 'eval_runtime': 7.1244, 'eval_samples_per_second': 12.212, 'eval_steps_per_second': 0.842, 'epoch': 0.22}
{'loss': 0.0262, 'grad_norm': 0.4605597257614136, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.2806708812713623, 'eval_runtime': 7.1365, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 0.841, 'epoch': 0.44}
{'loss': 0.0299, 'grad_norm': 0.42898982763290405, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.2806453704833984, 'eval_runtime': 7.1614, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 0.67}
{'loss': 0.0316, 'grad_norm': 0.2102281153202057, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.258052110671997, 'eval_runtime': 7.1365, 'eval_samples_per_second': 12.191, 'eval_steps_per_second': 0.841, 'epoch': 0.89}
{'loss': 0.0313, 'grad_norm': 0.23496080935001373, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.250251054763794, 'eval_runtime': 7.1934, 'eval_samples_per_second': 12.094, 'eval_steps_per_second': 0.834, 'epoch': 1.11}
{'loss': 0.0298, 'grad_norm': 0.22649037837982178, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.262613296508789, 'eval_runtime': 7.1584, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
{'loss': 0.0306, 'grad_norm': 0.22749976813793182, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.56}
{'eval_loss': 3.2618229389190674, 'eval_runtime': 7.1556, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.839, 'epoch': 1.56}
{'loss': 0.0306, 'grad_norm': 0.3169865310192108, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}
{'eval_loss': 3.2672371864318848, 'eval_runtime': 7.1528, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 1.78}
{'loss': 0.0337, 'grad_norm': 0.28044241666793823, 'learning_rate': 6e-05, 'epoch': 2.0}
{'eval_loss': 3.276151418685913, 'eval_runtime': 7.1446, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 2.0}
{'loss': 0.0344, 'grad_norm': 0.2905523180961609, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}
{'eval_loss': 3.256753921508789, 'eval_runtime': 7.1735, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 2.22}
{'loss': 0.0352, 'grad_norm': 0.36612072587013245, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.44}
{'eval_loss': 3.2482011318206787, 'eval_runtime': 7.1642, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.837, 'epoch': 2.44}
{'loss': 0.0382, 'grad_norm': 0.33827802538871765, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}
{'eval_loss': 3.2494430541992188, 'eval_runtime': 7.1677, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 2.67}
{'loss': 0.044, 'grad_norm': 0.5094457268714905, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.89}
{'eval_loss': 3.243396043777466, 'eval_runtime': 7.1334, 'eval_samples_per_second': 12.196, 'eval_steps_per_second': 0.841, 'epoch': 2.89}
{'loss': 0.044, 'grad_norm': 0.3346252739429474, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}
{'eval_loss': 3.2423996925354004, 'eval_runtime': 7.1381, 'eval_samples_per_second': 12.188, 'eval_steps_per_second': 0.841, 'epoch': 3.11}
{'loss': 0.0452, 'grad_norm': 0.27339187264442444, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}
{'eval_loss': 3.2451624870300293, 'eval_runtime': 7.1598, 'eval_samples_per_second': 12.151, 'eval_steps_per_second': 0.838, 'epoch': 3.33}
{'loss': 0.0501, 'grad_norm': 0.3230503499507904, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.2449257373809814, 'eval_runtime': 7.1611, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.0531, 'grad_norm': 0.2895418405532837, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.239532470703125, 'eval_runtime': 7.1318, 'eval_samples_per_second': 12.199, 'eval_steps_per_second': 0.841, 'epoch': 3.78}
{'loss': 0.0571, 'grad_norm': 0.33841776847839355, 'learning_rate': 2e-05, 'epoch': 4.0}
{'eval_loss': 3.230428457260132, 'eval_runtime': 7.1448, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 4.0}
{'loss': 0.0629, 'grad_norm': 0.4647989273071289, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.2249646186828613, 'eval_runtime': 7.147, 'eval_samples_per_second': 12.173, 'eval_steps_per_second': 0.84, 'epoch': 4.22}
{'loss': 0.072, 'grad_norm': 0.570697546005249, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
{'eval_loss': 3.2218055725097656, 'eval_runtime': 7.1544, 'eval_samples_per_second': 12.16, 'eval_steps_per_second': 0.839, 'epoch': 4.44}
{'loss': 0.0835, 'grad_norm': 0.4663720726966858, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.215615749359131, 'eval_runtime': 7.1635, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 4.67}
{'loss': 0.0931, 'grad_norm': 0.6482133865356445, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.2116787433624268, 'eval_runtime': 7.1407, 'eval_samples_per_second': 12.184, 'eval_steps_per_second': 0.84, 'epoch': 4.89}
{'train_runtime': 933.3043, 'train_samples_per_second': 3.857, 'train_steps_per_second': 0.241, 'train_loss': 0.045244940751128726, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd0.01...
Fine-tuning for learning_rate=0.0001 and weight_decay=0.01 complete. Model saved to './LlamaFinetuned_lr0.0001_wd0.01'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=0.005...
{'loss': 0.0054, 'grad_norm': 0.1827506273984909, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.291290521621704, 'eval_runtime': 7.138, 'eval_samples_per_second': 12.188, 'eval_steps_per_second': 0.841, 'epoch': 0.22}
{'loss': 0.023, 'grad_norm': 0.45153480768203735, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.2894535064697266, 'eval_runtime': 7.1604, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 0.44}
{'loss': 0.0269, 'grad_norm': 0.41408631205558777, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.288702964782715, 'eval_runtime': 7.1719, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 0.67}
{'loss': 0.0298, 'grad_norm': 0.23405678570270538, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.2698991298675537, 'eval_runtime': 7.1561, 'eval_samples_per_second': 12.157, 'eval_steps_per_second': 0.838, 'epoch': 0.89}
{'loss': 0.029, 'grad_norm': 0.2340371310710907, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.2596793174743652, 'eval_runtime': 7.1502, 'eval_samples_per_second': 12.167, 'eval_steps_per_second': 0.839, 'epoch': 1.11}
{'loss': 0.028, 'grad_norm': 0.2553492486476898, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.270524263381958, 'eval_runtime': 7.1613, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.0495, 'grad_norm': 0.3406924903392792, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.2521650791168213, 'eval_runtime': 7.1726, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.837, 'epoch': 3.56}
{'loss': 0.0527, 'grad_norm': 0.2922282814979553, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.246077299118042, 'eval_runtime': 7.1694, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 0.837, 'epoch': 3.78}
{'loss': 0.0569, 'grad_norm': 0.3611334264278412, 'learning_rate': 2e-05, 'epoch': 4.0}
{'eval_loss': 3.237251043319702, 'eval_runtime': 7.1753, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 4.0}
{'loss': 0.0629, 'grad_norm': 0.49449631571769714, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.2321560382843018, 'eval_runtime': 7.1571, 'eval_samples_per_second': 12.156, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.0723, 'grad_norm': 0.5985296964645386, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
{'eval_loss': 3.2292568683624268, 'eval_runtime': 7.1639, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 4.44}
{'loss': 0.0841, 'grad_norm': 0.48722249269485474, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.2228386402130127, 'eval_runtime': 7.1514, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 0.839, 'epoch': 4.67}
{'loss': 0.0944, 'grad_norm': 0.6792160868644714, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.2187552452087402, 'eval_runtime': 7.1419, 'eval_samples_per_second': 12.182, 'eval_steps_per_second': 0.84, 'epoch': 4.89}
{'train_runtime': 934.7906, 'train_samples_per_second': 3.851, 'train_steps_per_second': 0.241, 'train_loss': 0.0441790763537089, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd0.005...
Fine-tuning for learning_rate=0.0001 and weight_decay=0.005 complete. Model saved to './LlamaFinetuned_lr0.0001_wd0.005'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=0.001...
{'loss': 0.0044, 'grad_norm': 0.16060365736484528, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.3027048110961914, 'eval_runtime': 7.1626, 'eval_samples_per_second': 12.146, 'eval_steps_per_second': 0.838, 'epoch': 0.22}
{'loss': 0.02, 'grad_norm': 0.44056257605552673, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.2988839149475098, 'eval_runtime': 7.1318, 'eval_samples_per_second': 12.199, 'eval_steps_per_second': 0.841, 'epoch': 0.44}
{'loss': 0.0242, 'grad_norm': 0.4110705554485321, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.296478271484375, 'eval_runtime': 7.1666, 'eval_samples_per_second': 12.14, 'eval_steps_per_second': 0.837, 'epoch': 0.67}
{'loss': 0.0279, 'grad_norm': 0.28165683150291443, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.281855821609497, 'eval_runtime': 7.1679, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 0.89}
{'loss': 0.0269, 'grad_norm': 0.24841010570526123, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.2680389881134033, 'eval_runtime': 7.1625, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 1.11}
{'loss': 0.0262, 'grad_norm': 0.29527392983436584, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.2788732051849365, 'eval_runtime': 7.1424, 'eval_samples_per_second': 12.181, 'eval_steps_per_second': 0.84, 'epoch': 1.33}
{'loss': 0.027, 'grad_norm': 0.28961580991744995, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.56}
{'eval_loss': 3.2795281410217285, 'eval_runtime': 7.1369, 'eval_samples_per_second': 12.19, 'eval_steps_per_second': 0.841, 'epoch': 1.56}
{'loss': 0.0267, 'grad_norm': 0.33876484632492065, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}
{'eval_loss': 3.2867350578308105, 'eval_runtime': 7.1386, 'eval_samples_per_second': 12.187, 'eval_steps_per_second': 0.841, 'epoch': 1.78}
{'loss': 0.03, 'grad_norm': 0.31086647510528564, 'learning_rate': 6e-05, 'epoch': 2.0}
{'eval_loss': 3.2963061332702637, 'eval_runtime': 7.1765, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 2.0}
{'loss': 0.0314, 'grad_norm': 0.31612908840179443, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}
{'eval_loss': 3.2776427268981934, 'eval_runtime': 7.165, 'eval_samples_per_second': 12.142, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.0325, 'grad_norm': 0.4203953146934509, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.44}
{'eval_loss': 3.2679965496063232, 'eval_runtime': 7.1521, 'eval_samples_per_second': 12.164, 'eval_steps_per_second': 0.839, 'epoch': 2.44}
{'loss': 0.0354, 'grad_norm': 0.3561111390590668, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}
{'eval_loss': 3.267183542251587, 'eval_runtime': 7.1475, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 0.839, 'epoch': 2.67}
{'loss': 0.0418, 'grad_norm': 0.5614608526229858, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.89}
{'eval_loss': 3.2585134506225586, 'eval_runtime': 7.1589, 'eval_samples_per_second': 12.153, 'eval_steps_per_second': 0.838, 'epoch': 2.89}
{'loss': 0.042, 'grad_norm': 0.35396215319633484, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}
{'eval_loss': 3.258254051208496, 'eval_runtime': 7.1559, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.838, 'epoch': 3.11}
{'loss': 0.0439, 'grad_norm': 0.28779253363609314, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}
{'eval_loss': 3.2626798152923584, 'eval_runtime': 7.1438, 'eval_samples_per_second': 12.178, 'eval_steps_per_second': 0.84, 'epoch': 3.33}
{'loss': 0.0488, 'grad_norm': 0.35750025510787964, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.2587199211120605, 'eval_runtime': 7.1338, 'eval_samples_per_second': 12.195, 'eval_steps_per_second': 0.841, 'epoch': 3.56}
{'loss': 0.0524, 'grad_norm': 0.29545363783836365, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.252380132675171, 'eval_runtime': 7.1838, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 3.78}
{'loss': 0.0566, 'grad_norm': 0.38225191831588745, 'learning_rate': 2e-05, 'epoch': 4.0}
{'eval_loss': 3.243729591369629, 'eval_runtime': 7.1583, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 0.838, 'epoch': 4.0}
{'loss': 0.063, 'grad_norm': 0.5218207240104675, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.239128828048706, 'eval_runtime': 7.1568, 'eval_samples_per_second': 12.156, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.0727, 'grad_norm': 0.6252152323722839, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
{'eval_loss': 3.2363927364349365, 'eval_runtime': 7.1751, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 4.44}
{'loss': 0.0846, 'grad_norm': 0.5059545040130615, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.229775905609131, 'eval_runtime': 7.1572, 'eval_samples_per_second': 12.156, 'eval_steps_per_second': 0.838, 'epoch': 4.67}
{'loss': 0.0956, 'grad_norm': 0.7091885209083557, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.225532293319702, 'eval_runtime': 7.1621, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 4.89}
{'train_runtime': 933.8374, 'train_samples_per_second': 3.855, 'train_steps_per_second': 0.241, 'train_loss': 0.04314467594027519, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd0.001...
Fine-tuning for learning_rate=0.0001 and weight_decay=0.001 complete. Model saved to './LlamaFinetuned_lr0.0001_wd0.001'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=0.0005...
{'loss': 0.0037, 'grad_norm': 0.14305779337882996, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.3128015995025635, 'eval_runtime': 7.1675, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.837, 'epoch': 0.22}
{'loss': 0.0172, 'grad_norm': 0.43179091811180115, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.308685064315796, 'eval_runtime': 7.1725, 'eval_samples_per_second': 12.13, 'eval_steps_per_second': 0.837, 'epoch': 0.44}
{'loss': 0.0217, 'grad_norm': 0.4075354337692261, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.304405450820923, 'eval_runtime': 7.1195, 'eval_samples_per_second': 12.22, 'eval_steps_per_second': 0.843, 'epoch': 0.67}
{'loss': 0.026, 'grad_norm': 0.3259011209011078, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.2950334548950195, 'eval_runtime': 7.1577, 'eval_samples_per_second': 12.155, 'eval_steps_per_second': 0.838, 'epoch': 0.89}
{'loss': 0.0248, 'grad_norm': 0.2677544057369232, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.2772388458251953, 'eval_runtime': 7.1496, 'eval_samples_per_second': 12.168, 'eval_steps_per_second': 0.839, 'epoch': 1.11}
{'loss': 0.0243, 'grad_norm': 0.3333989679813385, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.289013147354126, 'eval_runtime': 7.1648, 'eval_samples_per_second': 12.143, 'eval_steps_per_second': 0.837, 'epoch': 1.33}
{'loss': 0.0253, 'grad_norm': 0.3065873980522156, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.56}
{'eval_loss': 3.289992332458496, 'eval_runtime': 7.1545, 'eval_samples_per_second': 12.16, 'eval_steps_per_second': 0.839, 'epoch': 1.56}
{'loss': 0.025, 'grad_norm': 0.34557831287384033, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}
{'eval_loss': 3.2972145080566406, 'eval_runtime': 7.1574, 'eval_samples_per_second': 12.155, 'eval_steps_per_second': 0.838, 'epoch': 1.78}
{'loss': 0.0281, 'grad_norm': 0.3419748842716217, 'learning_rate': 6e-05, 'epoch': 2.0}
{'eval_loss': 3.3066794872283936, 'eval_runtime': 7.1621, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 2.0}
{'loss': 0.0299, 'grad_norm': 0.3426631689071655, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}
{'eval_loss': 3.2867298126220703, 'eval_runtime': 7.1513, 'eval_samples_per_second': 12.166, 'eval_steps_per_second': 0.839, 'epoch': 2.22}
{'loss': 0.0311, 'grad_norm': 0.42783957719802856, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.44}
{'eval_loss': 3.2766103744506836, 'eval_runtime': 7.1515, 'eval_samples_per_second': 12.165, 'eval_steps_per_second': 0.839, 'epoch': 2.44}
{'loss': 0.034, 'grad_norm': 0.35365423560142517, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}
{'eval_loss': 3.2761363983154297, 'eval_runtime': 7.1441, 'eval_samples_per_second': 12.178, 'eval_steps_per_second': 0.84, 'epoch': 2.67}
{'loss': 0.0408, 'grad_norm': 0.5804553031921387, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.89}
{'eval_loss': 3.265557289123535, 'eval_runtime': 7.1961, 'eval_samples_per_second': 12.09, 'eval_steps_per_second': 0.834, 'epoch': 2.89}
{'loss': 0.0411, 'grad_norm': 0.36260831356048584, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}
{'eval_loss': 3.2657558917999268, 'eval_runtime': 7.1447, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 3.11}
{'loss': 0.0432, 'grad_norm': 0.30375248193740845, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}
{'eval_loss': 3.2715156078338623, 'eval_runtime': 7.1539, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 3.33}
{'loss': 0.0481, 'grad_norm': 0.3713487684726715, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.2651283740997314, 'eval_runtime': 7.156, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.052, 'grad_norm': 0.29972565174102783, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.2589564323425293, 'eval_runtime': 7.1532, 'eval_samples_per_second': 12.162, 'eval_steps_per_second': 0.839, 'epoch': 3.78}
{'loss': 0.0564, 'grad_norm': 0.40256309509277344, 'learning_rate': 2e-05, 'epoch': 4.0}
{'eval_loss': 3.2502479553222656, 'eval_runtime': 7.1313, 'eval_samples_per_second': 12.2, 'eval_steps_per_second': 0.841, 'epoch': 4.0}
{'loss': 0.063, 'grad_norm': 0.5480247139930725, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.2461249828338623, 'eval_runtime': 7.1146, 'eval_samples_per_second': 12.228, 'eval_steps_per_second': 0.843, 'epoch': 4.22}
{'loss': 0.0729, 'grad_norm': 0.6504612565040588, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
  return fn(*args, **kwargs)█████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)█████████████████████████████████████████████████████████████                                                        | 135/225 [09:18<05:55,  3.95s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.0852, 'grad_norm': 0.5233291387557983, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.2366740703582764, 'eval_runtime': 7.1423, 'eval_samples_per_second': 12.181, 'eval_steps_per_second': 0.84, 'epoch': 4.67}
{'loss': 0.0968, 'grad_norm': 0.7403358221054077, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.23228120803833, 'eval_runtime': 7.1475, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 0.839, 'epoch': 4.89}
{'train_runtime': 933.7303, 'train_samples_per_second': 3.856, 'train_steps_per_second': 0.241, 'train_loss': 0.042138429151640996, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd0.0005...
Fine-tuning for learning_rate=0.0001 and weight_decay=0.0005 complete. Model saved to './LlamaFinetuned_lr0.0001_wd0.0005'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=0.0001...
{'loss': 0.003, 'grad_norm': 0.13116511702537537, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.322352886199951, 'eval_runtime': 7.1476, 'eval_samples_per_second': 12.172, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.0147, 'grad_norm': 0.42235854268074036, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.318514347076416, 'eval_runtime': 7.1391, 'eval_samples_per_second': 12.186, 'eval_steps_per_second': 0.84, 'epoch': 0.44}
{'loss': 0.0193, 'grad_norm': 0.3822389841079712, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.312530517578125, 'eval_runtime': 7.1559, 'eval_samples_per_second': 12.158, 'eval_steps_per_second': 0.838, 'epoch': 0.67}
{'loss': 0.0238, 'grad_norm': 0.32367050647735596, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.309016466140747, 'eval_runtime': 7.1743, 'eval_samples_per_second': 12.127, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 0.0227, 'grad_norm': 0.27689898014068604, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.287245512008667, 'eval_runtime': 7.1447, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 1.11}
{'loss': 0.0225, 'grad_norm': 0.3401898145675659, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.2994205951690674, 'eval_runtime': 7.1761, 'eval_samples_per_second': 12.124, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.0236, 'grad_norm': 0.332937091588974, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.56}
{'eval_loss': 3.3006834983825684, 'eval_runtime': 7.1712, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 1.56}
{'loss': 0.0233, 'grad_norm': 0.3447361886501312, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}
{'eval_loss': 3.308736801147461, 'eval_runtime': 7.1461, 'eval_samples_per_second': 12.174, 'eval_steps_per_second': 0.84, 'epoch': 1.78}
{'loss': 0.0261, 'grad_norm': 0.3825409710407257, 'learning_rate': 6e-05, 'epoch': 2.0}
{'eval_loss': 3.316884756088257, 'eval_runtime': 7.1671, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 2.0}
{'loss': 0.0282, 'grad_norm': 0.376669317483902, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}
{'eval_loss': 3.2954013347625732, 'eval_runtime': 7.171, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.0297, 'grad_norm': 0.4369659423828125, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.44}
{'eval_loss': 3.2847869396209717, 'eval_runtime': 7.1736, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 2.44}
{'loss': 0.0327, 'grad_norm': 0.3416825234889984, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}
{'eval_loss': 3.2850284576416016, 'eval_runtime': 7.1771, 'eval_samples_per_second': 12.122, 'eval_steps_per_second': 0.836, 'epoch': 2.67}
{'loss': 0.0397, 'grad_norm': 0.588696300983429, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.89}
{'eval_loss': 3.2727608680725098, 'eval_runtime': 7.1699, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.0401, 'grad_norm': 0.365867018699646, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}
{'eval_loss': 3.2730298042297363, 'eval_runtime': 7.1602, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 3.11}
{'loss': 0.0425, 'grad_norm': 0.3282395303249359, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}
{'eval_loss': 3.2800567150115967, 'eval_runtime': 7.17, 'eval_samples_per_second': 12.134, 'eval_steps_per_second': 0.837, 'epoch': 3.33}
{'loss': 0.0473, 'grad_norm': 0.3814987540245056, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.2714853286743164, 'eval_runtime': 7.1658, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 3.56}
{'loss': 0.0517, 'grad_norm': 0.3078564405441284, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.2656443119049072, 'eval_runtime': 7.1739, 'eval_samples_per_second': 12.127, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.0562, 'grad_norm': 0.42214053869247437, 'learning_rate': 2e-05, 'epoch': 4.0}
{'eval_loss': 3.256667137145996, 'eval_runtime': 7.1376, 'eval_samples_per_second': 12.189, 'eval_steps_per_second': 0.841, 'epoch': 4.0}
{'loss': 0.063, 'grad_norm': 0.5730584859848022, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.2530062198638916, 'eval_runtime': 7.1637, 'eval_samples_per_second': 12.145, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.0732, 'grad_norm': 0.6751143932342529, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
{'eval_loss': 3.2504026889801025, 'eval_runtime': 7.1376, 'eval_samples_per_second': 12.189, 'eval_steps_per_second': 0.841, 'epoch': 4.44}
{'loss': 0.0857, 'grad_norm': 0.5393893718719482, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.243464946746826, 'eval_runtime': 7.1684, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 4.67}
{'loss': 0.098, 'grad_norm': 0.773130476474762, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.2389252185821533, 'eval_runtime': 7.1754, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 4.89}
{'train_runtime': 933.875, 'train_samples_per_second': 3.855, 'train_steps_per_second': 0.241, 'train_loss': 0.04113559876879056, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd0.0001...
Fine-tuning for learning_rate=0.0001 and weight_decay=0.0001 complete. Model saved to './LlamaFinetuned_lr0.0001_wd0.0001'.
Starting fine-tuning for learning_rate=0.0001 and weight_decay=1e-05...
{'loss': 0.0024, 'grad_norm': 0.12342248111963272, 'learning_rate': 9.555555555555557e-05, 'epoch': 0.22}
{'eval_loss': 3.3328864574432373, 'eval_runtime': 7.136, 'eval_samples_per_second': 12.192, 'eval_steps_per_second': 0.841, 'epoch': 0.22}
{'loss': 0.0124, 'grad_norm': 0.4192837178707123, 'learning_rate': 9.111111111111112e-05, 'epoch': 0.44}
{'eval_loss': 3.3281078338623047, 'eval_runtime': 7.1519, 'eval_samples_per_second': 12.165, 'eval_steps_per_second': 0.839, 'epoch': 0.44}
{'loss': 0.0171, 'grad_norm': 0.3356059193611145, 'learning_rate': 8.666666666666667e-05, 'epoch': 0.67}
{'eval_loss': 3.320474624633789, 'eval_runtime': 7.1445, 'eval_samples_per_second': 12.177, 'eval_steps_per_second': 0.84, 'epoch': 0.67}
{'loss': 0.0216, 'grad_norm': 0.27455779910087585, 'learning_rate': 8.222222222222222e-05, 'epoch': 0.89}
{'eval_loss': 3.322472333908081, 'eval_runtime': 7.1485, 'eval_samples_per_second': 12.17, 'eval_steps_per_second': 0.839, 'epoch': 0.89}
{'loss': 0.0206, 'grad_norm': 0.28588244318962097, 'learning_rate': 7.777777777777778e-05, 'epoch': 1.11}
{'eval_loss': 3.2968215942382812, 'eval_runtime': 7.1483, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.839, 'epoch': 1.11}
{'loss': 0.0206, 'grad_norm': 0.3155983090400696, 'learning_rate': 7.333333333333333e-05, 'epoch': 1.33}
{'eval_loss': 3.3077335357666016, 'eval_runtime': 7.1641, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
{'loss': 0.0218, 'grad_norm': 0.3484008014202118, 'learning_rate': 6.88888888888889e-05, 'epoch': 1.56}
{'eval_loss': 3.3099493980407715, 'eval_runtime': 7.1542, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 1.56}
{'loss': 0.0217, 'grad_norm': 0.3492129445075989, 'learning_rate': 6.444444444444446e-05, 'epoch': 1.78}
{'eval_loss': 3.319596290588379, 'eval_runtime': 7.1515, 'eval_samples_per_second': 12.165, 'eval_steps_per_second': 0.839, 'epoch': 1.78}
{'loss': 0.0241, 'grad_norm': 0.415383517742157, 'learning_rate': 6e-05, 'epoch': 2.0}
{'eval_loss': 3.326479196548462, 'eval_runtime': 7.1614, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 2.0}
{'loss': 0.0266, 'grad_norm': 0.399601012468338, 'learning_rate': 5.555555555555556e-05, 'epoch': 2.22}
{'eval_loss': 3.303894281387329, 'eval_runtime': 7.143, 'eval_samples_per_second': 12.18, 'eval_steps_per_second': 0.84, 'epoch': 2.22}
{'loss': 0.0282, 'grad_norm': 0.45054611563682556, 'learning_rate': 5.111111111111111e-05, 'epoch': 2.44}
{'eval_loss': 3.2926340103149414, 'eval_runtime': 7.1871, 'eval_samples_per_second': 12.105, 'eval_steps_per_second': 0.835, 'epoch': 2.44}
{'loss': 0.0315, 'grad_norm': 0.32162800431251526, 'learning_rate': 4.666666666666667e-05, 'epoch': 2.67}
{'eval_loss': 3.2932138442993164, 'eval_runtime': 7.1581, 'eval_samples_per_second': 12.154, 'eval_steps_per_second': 0.838, 'epoch': 2.67}
{'loss': 0.0387, 'grad_norm': 0.5924880504608154, 'learning_rate': 4.222222222222222e-05, 'epoch': 2.89}
{'eval_loss': 3.2799124717712402, 'eval_runtime': 7.168, 'eval_samples_per_second': 12.137, 'eval_steps_per_second': 0.837, 'epoch': 2.89}
{'loss': 0.0391, 'grad_norm': 0.3698221743106842, 'learning_rate': 3.777777777777778e-05, 'epoch': 3.11}
{'eval_loss': 3.279658079147339, 'eval_runtime': 7.1569, 'eval_samples_per_second': 12.156, 'eval_steps_per_second': 0.838, 'epoch': 3.11}
{'loss': 0.0419, 'grad_norm': 0.35800352692604065, 'learning_rate': 3.3333333333333335e-05, 'epoch': 3.33}
{'eval_loss': 3.287536382675171, 'eval_runtime': 7.1812, 'eval_samples_per_second': 12.115, 'eval_steps_per_second': 0.836, 'epoch': 3.33}
{'loss': 0.0465, 'grad_norm': 0.3894960880279541, 'learning_rate': 2.8888888888888888e-05, 'epoch': 3.56}
{'eval_loss': 3.277660846710205, 'eval_runtime': 7.1427, 'eval_samples_per_second': 12.18, 'eval_steps_per_second': 0.84, 'epoch': 3.56}
{'loss': 0.0514, 'grad_norm': 0.3214977979660034, 'learning_rate': 2.4444444444444445e-05, 'epoch': 3.78}
{'eval_loss': 3.271942138671875, 'eval_runtime': 7.1736, 'eval_samples_per_second': 12.128, 'eval_steps_per_second': 0.836, 'epoch': 3.78}
{'loss': 0.0559, 'grad_norm': 0.4412323534488678, 'learning_rate': 2e-05, 'epoch': 4.0}
{'eval_loss': 3.262920618057251, 'eval_runtime': 7.1669, 'eval_samples_per_second': 12.139, 'eval_steps_per_second': 0.837, 'epoch': 4.0}
{'loss': 0.063, 'grad_norm': 0.5960485339164734, 'learning_rate': 1.5555555555555555e-05, 'epoch': 4.22}
{'eval_loss': 3.259842872619629, 'eval_runtime': 7.1615, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.0733, 'grad_norm': 0.6974071860313416, 'learning_rate': 1.1111111111111112e-05, 'epoch': 4.44}
{'eval_loss': 3.2572319507598877, 'eval_runtime': 7.177, 'eval_samples_per_second': 12.122, 'eval_steps_per_second': 0.836, 'epoch': 4.44}
{'loss': 0.0862, 'grad_norm': 0.5527756810188293, 'learning_rate': 6.666666666666667e-06, 'epoch': 4.67}
{'eval_loss': 3.250145196914673, 'eval_runtime': 7.1765, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.0992, 'grad_norm': 0.8070292472839355, 'learning_rate': 2.2222222222222225e-06, 'epoch': 4.89}
{'eval_loss': 3.2454771995544434, 'eval_runtime': 7.1458, 'eval_samples_per_second': 12.175, 'eval_steps_per_second': 0.84, 'epoch': 4.89}
{'train_runtime': 934.4936, 'train_samples_per_second': 3.852, 'train_steps_per_second': 0.241, 'train_loss': 0.040126268772615326, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr0.0001_wd1e-05...
Fine-tuning for learning_rate=0.0001 and weight_decay=1e-05 complete. Model saved to './LlamaFinetuned_lr0.0001_wd1e-05'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=0.1...
{'loss': 0.0064, 'grad_norm': 0.43236690759658813, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.258877754211426, 'eval_runtime': 7.1528, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 0.22}
{'loss': 0.0157, 'grad_norm': 0.19437795877456665, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.263951063156128, 'eval_runtime': 7.1531, 'eval_samples_per_second': 12.163, 'eval_steps_per_second': 0.839, 'epoch': 0.44}
{'loss': 0.0181, 'grad_norm': 0.3066955506801605, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.2667770385742188, 'eval_runtime': 7.1519, 'eval_samples_per_second': 12.165, 'eval_steps_per_second': 0.839, 'epoch': 0.67}
{'loss': 0.0211, 'grad_norm': 0.2980879843235016, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.26863956451416, 'eval_runtime': 7.159, 'eval_samples_per_second': 12.152, 'eval_steps_per_second': 0.838, 'epoch': 0.89}
{'loss': 0.0209, 'grad_norm': 0.268017441034317, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.2673702239990234, 'eval_runtime': 7.1539, 'eval_samples_per_second': 12.161, 'eval_steps_per_second': 0.839, 'epoch': 1.11}
{'loss': 0.0206, 'grad_norm': 0.2028651386499405, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.269669532775879, 'eval_runtime': 7.1605, 'eval_samples_per_second': 12.15, 'eval_steps_per_second': 0.838, 'epoch': 1.33}
{'loss': 0.0207, 'grad_norm': 0.25187671184539795, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
{'eval_loss': 3.2724177837371826, 'eval_runtime': 7.1744, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 1.56}
{'loss': 0.0217, 'grad_norm': 0.35197561979293823, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.274080514907837, 'eval_runtime': 7.1845, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 1.78}
{'loss': 0.0234, 'grad_norm': 0.3641071021556854, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.277229070663452, 'eval_runtime': 7.1847, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 2.0}
{'loss': 0.0254, 'grad_norm': 0.34568411111831665, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.27848482131958, 'eval_runtime': 7.1483, 'eval_samples_per_second': 12.171, 'eval_steps_per_second': 0.839, 'epoch': 2.22}
{'loss': 0.0256, 'grad_norm': 0.35059234499931335, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.277463436126709, 'eval_runtime': 7.131, 'eval_samples_per_second': 12.2, 'eval_steps_per_second': 0.841, 'epoch': 2.44}
{'loss': 0.0285, 'grad_norm': 0.367606520652771, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.277540922164917, 'eval_runtime': 7.1776, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 2.67}
{'loss': 0.035, 'grad_norm': 0.4240163266658783, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.2784056663513184, 'eval_runtime': 7.1624, 'eval_samples_per_second': 12.147, 'eval_steps_per_second': 0.838, 'epoch': 2.89}
{'loss': 0.0361, 'grad_norm': 0.35445350408554077, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.276751756668091, 'eval_runtime': 7.1506, 'eval_samples_per_second': 12.167, 'eval_steps_per_second': 0.839, 'epoch': 3.11}
{'loss': 0.0395, 'grad_norm': 0.3650655448436737, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.275867223739624, 'eval_runtime': 7.1694, 'eval_samples_per_second': 12.135, 'eval_steps_per_second': 0.837, 'epoch': 3.33}
{'loss': 0.0426, 'grad_norm': 0.3914707899093628, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 135/225 [09:19<05:56,  3.96s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.0488, 'grad_norm': 0.38405847549438477, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.2750697135925293, 'eval_runtime': 7.1638, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 3.78}
{'loss': 0.0542, 'grad_norm': 0.40231892466545105, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.273599624633789, 'eval_runtime': 7.19, 'eval_samples_per_second': 12.1, 'eval_steps_per_second': 0.834, 'epoch': 4.0}
{'loss': 0.0618, 'grad_norm': 0.6254287958145142, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.272588014602661, 'eval_runtime': 7.1572, 'eval_samples_per_second': 12.156, 'eval_steps_per_second': 0.838, 'epoch': 4.22}
{'loss': 0.0732, 'grad_norm': 0.8121141791343689, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.2719364166259766, 'eval_runtime': 7.1827, 'eval_samples_per_second': 12.113, 'eval_steps_per_second': 0.835, 'epoch': 4.44}
{'loss': 0.0862, 'grad_norm': 0.6536367535591125, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.271143674850464, 'eval_runtime': 7.1687, 'eval_samples_per_second': 12.136, 'eval_steps_per_second': 0.837, 'epoch': 4.67}
{'loss': 0.1023, 'grad_norm': 0.8220998048782349, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
{'eval_loss': 3.270580768585205, 'eval_runtime': 7.1717, 'eval_samples_per_second': 12.131, 'eval_steps_per_second': 0.837, 'epoch': 4.89}
{'train_runtime': 934.4087, 'train_samples_per_second': 3.853, 'train_steps_per_second': 0.241, 'train_loss': 0.039506126244862876, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd0.1...
Fine-tuning for learning_rate=1e-05 and weight_decay=0.1 complete. Model saved to './LlamaFinetuned_lr1e-05_wd0.1'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=0.05...
{'loss': 0.0041, 'grad_norm': 0.47600501775741577, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.2832841873168945, 'eval_runtime': 7.1764, 'eval_samples_per_second': 12.123, 'eval_steps_per_second': 0.836, 'epoch': 0.22}
{'loss': 0.0124, 'grad_norm': 0.20183038711547852, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.285618543624878, 'eval_runtime': 7.1934, 'eval_samples_per_second': 12.094, 'eval_steps_per_second': 0.834, 'epoch': 0.44}
{'loss': 0.0156, 'grad_norm': 0.36930257081985474, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.2882578372955322, 'eval_runtime': 7.1911, 'eval_samples_per_second': 12.098, 'eval_steps_per_second': 0.834, 'epoch': 0.67}
{'loss': 0.0194, 'grad_norm': 0.31825578212738037, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.289159059524536, 'eval_runtime': 7.1627, 'eval_samples_per_second': 12.146, 'eval_steps_per_second': 0.838, 'epoch': 0.89}
{'loss': 0.0195, 'grad_norm': 0.26238077878952026, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.2870001792907715, 'eval_runtime': 7.196, 'eval_samples_per_second': 12.09, 'eval_steps_per_second': 0.834, 'epoch': 1.11}
{'loss': 0.0193, 'grad_norm': 0.23518043756484985, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.288210868835449, 'eval_runtime': 7.1918, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 1.33}
{'loss': 0.0191, 'grad_norm': 0.2821837365627289, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
{'eval_loss': 3.2898075580596924, 'eval_runtime': 7.186, 'eval_samples_per_second': 12.107, 'eval_steps_per_second': 0.835, 'epoch': 1.56}
{'loss': 0.0203, 'grad_norm': 0.38948050141334534, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.2905941009521484, 'eval_runtime': 7.1759, 'eval_samples_per_second': 12.124, 'eval_steps_per_second': 0.836, 'epoch': 1.78}
{'loss': 0.0222, 'grad_norm': 0.39411720633506775, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.2929413318634033, 'eval_runtime': 7.1898, 'eval_samples_per_second': 12.1, 'eval_steps_per_second': 0.835, 'epoch': 2.0}
{'loss': 0.0246, 'grad_norm': 0.39733758568763733, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.2935867309570312, 'eval_runtime': 7.1969, 'eval_samples_per_second': 12.089, 'eval_steps_per_second': 0.834, 'epoch': 2.22}
{'loss': 0.0253, 'grad_norm': 0.37740567326545715, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.2918717861175537, 'eval_runtime': 7.1746, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 2.44}
{'loss': 0.0285, 'grad_norm': 0.40365105867385864, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.291170597076416, 'eval_runtime': 7.1875, 'eval_samples_per_second': 12.104, 'eval_steps_per_second': 0.835, 'epoch': 2.67}
{'loss': 0.0351, 'grad_norm': 0.46180012822151184, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.2914795875549316, 'eval_runtime': 7.1886, 'eval_samples_per_second': 12.102, 'eval_steps_per_second': 0.835, 'epoch': 2.89}
{'loss': 0.0364, 'grad_norm': 0.3855915665626526, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.2895374298095703, 'eval_runtime': 7.1832, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 3.11}
{'loss': 0.0401, 'grad_norm': 0.39431026577949524, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.2883644104003906, 'eval_runtime': 7.1609, 'eval_samples_per_second': 12.149, 'eval_steps_per_second': 0.838, 'epoch': 3.33}
{'loss': 0.0434, 'grad_norm': 0.42104071378707886, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
{'eval_loss': 3.287771701812744, 'eval_runtime': 7.1875, 'eval_samples_per_second': 12.104, 'eval_steps_per_second': 0.835, 'epoch': 3.56}
{'loss': 0.0496, 'grad_norm': 0.4125082194805145, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.2868926525115967, 'eval_runtime': 7.1898, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 3.78}
{'loss': 0.0552, 'grad_norm': 0.4337572753429413, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.285316228866577, 'eval_runtime': 7.1804, 'eval_samples_per_second': 12.116, 'eval_steps_per_second': 0.836, 'epoch': 4.0}
{'loss': 0.0628, 'grad_norm': 0.6548689603805542, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.2841713428497314, 'eval_runtime': 7.1855, 'eval_samples_per_second': 12.108, 'eval_steps_per_second': 0.835, 'epoch': 4.22}
{'loss': 0.0746, 'grad_norm': 0.8787814974784851, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.283437728881836, 'eval_runtime': 7.1849, 'eval_samples_per_second': 12.109, 'eval_steps_per_second': 0.835, 'epoch': 4.44}
{'loss': 0.0878, 'grad_norm': 0.6809574365615845, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.2825870513916016, 'eval_runtime': 7.175, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.104, 'grad_norm': 0.8621566891670227, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
{'eval_loss': 3.2819933891296387, 'eval_runtime': 7.1827, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 4.89}
{'train_runtime': 936.4237, 'train_samples_per_second': 3.844, 'train_steps_per_second': 0.24, 'train_loss': 0.03916349141134156, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd0.05...
Fine-tuning for learning_rate=1e-05 and weight_decay=0.05 complete. Model saved to './LlamaFinetuned_lr1e-05_wd0.05'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=0.01...
{'loss': 0.0037, 'grad_norm': 0.46962571144104004, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.2940571308135986, 'eval_runtime': 7.1639, 'eval_samples_per_second': 12.144, 'eval_steps_per_second': 0.838, 'epoch': 0.22}
{'loss': 0.0113, 'grad_norm': 0.2122591733932495, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.295734167098999, 'eval_runtime': 7.1901, 'eval_samples_per_second': 12.1, 'eval_steps_per_second': 0.834, 'epoch': 0.44}
{'loss': 0.0147, 'grad_norm': 0.38603270053863525, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.2978508472442627, 'eval_runtime': 7.1969, 'eval_samples_per_second': 12.089, 'eval_steps_per_second': 0.834, 'epoch': 0.67}
{'loss': 0.0185, 'grad_norm': 0.3226030468940735, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.2982239723205566, 'eval_runtime': 7.1749, 'eval_samples_per_second': 12.126, 'eval_steps_per_second': 0.836, 'epoch': 0.89}
{'loss': 0.0187, 'grad_norm': 0.26747795939445496, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.2956161499023438, 'eval_runtime': 7.1615, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 1.11}
{'loss': 0.0187, 'grad_norm': 0.2526930570602417, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.29641056060791, 'eval_runtime': 7.1755, 'eval_samples_per_second': 12.125, 'eval_steps_per_second': 0.836, 'epoch': 1.33}
{'loss': 0.0185, 'grad_norm': 0.2960868775844574, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
{'eval_loss': 3.297598361968994, 'eval_runtime': 7.18, 'eval_samples_per_second': 12.117, 'eval_steps_per_second': 0.836, 'epoch': 1.56}
{'loss': 0.0198, 'grad_norm': 0.40580669045448303, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.2980494499206543, 'eval_runtime': 7.1918, 'eval_samples_per_second': 12.097, 'eval_steps_per_second': 0.834, 'epoch': 1.78}
{'loss': 0.0217, 'grad_norm': 0.4002150595188141, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.3001670837402344, 'eval_runtime': 7.1964, 'eval_samples_per_second': 12.089, 'eval_steps_per_second': 0.834, 'epoch': 2.0}
{'loss': 0.0243, 'grad_norm': 0.4197341203689575, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.3006062507629395, 'eval_runtime': 7.1658, 'eval_samples_per_second': 12.141, 'eval_steps_per_second': 0.837, 'epoch': 2.22}
{'loss': 0.0251, 'grad_norm': 0.38792479038238525, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.298546075820923, 'eval_runtime': 7.1835, 'eval_samples_per_second': 12.111, 'eval_steps_per_second': 0.835, 'epoch': 2.44}
{'loss': 0.0284, 'grad_norm': 0.41902148723602295, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.297548532485962, 'eval_runtime': 7.1829, 'eval_samples_per_second': 12.112, 'eval_steps_per_second': 0.835, 'epoch': 2.67}
{'loss': 0.0352, 'grad_norm': 0.47969210147857666, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.2976396083831787, 'eval_runtime': 7.1895, 'eval_samples_per_second': 12.101, 'eval_steps_per_second': 0.835, 'epoch': 2.89}
{'loss': 0.0365, 'grad_norm': 0.3991222381591797, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.2955918312072754, 'eval_runtime': 7.1775, 'eval_samples_per_second': 12.121, 'eval_steps_per_second': 0.836, 'epoch': 3.11}
{'loss': 0.0404, 'grad_norm': 0.40307798981666565, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.294321060180664, 'eval_runtime': 7.1987, 'eval_samples_per_second': 12.086, 'eval_steps_per_second': 0.833, 'epoch': 3.33}
{'loss': 0.0437, 'grad_norm': 0.4327821135520935, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
{'eval_loss': 3.2935867309570312, 'eval_runtime': 7.1619, 'eval_samples_per_second': 12.148, 'eval_steps_per_second': 0.838, 'epoch': 3.56}
{'loss': 0.0501, 'grad_norm': 0.42673438787460327, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.2926650047302246, 'eval_runtime': 7.1861, 'eval_samples_per_second': 12.107, 'eval_steps_per_second': 0.835, 'epoch': 3.78}
{'loss': 0.0556, 'grad_norm': 0.45135241746902466, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.291043758392334, 'eval_runtime': 7.1756, 'eval_samples_per_second': 12.124, 'eval_steps_per_second': 0.836, 'epoch': 4.0}
{'loss': 0.0633, 'grad_norm': 0.6642352342605591, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.289853811264038, 'eval_runtime': 7.1738, 'eval_samples_per_second': 12.127, 'eval_steps_per_second': 0.836, 'epoch': 4.22}
{'loss': 0.0753, 'grad_norm': 0.910783052444458, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.289104700088501, 'eval_runtime': 7.1952, 'eval_samples_per_second': 12.091, 'eval_steps_per_second': 0.834, 'epoch': 4.44}
{'loss': 0.0886, 'grad_norm': 0.6982139348983765, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.288231134414673, 'eval_runtime': 7.1731, 'eval_samples_per_second': 12.129, 'eval_steps_per_second': 0.836, 'epoch': 4.67}
{'loss': 0.1048, 'grad_norm': 0.8830025792121887, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
{'eval_loss': 3.2876229286193848, 'eval_runtime': 7.2001, 'eval_samples_per_second': 12.083, 'eval_steps_per_second': 0.833, 'epoch': 4.89}
{'train_runtime': 936.8844, 'train_samples_per_second': 3.843, 'train_steps_per_second': 0.24, 'train_loss': 0.03908261064026091, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd0.01...
Fine-tuning for learning_rate=1e-05 and weight_decay=0.01 complete. Model saved to './LlamaFinetuned_lr1e-05_wd0.01'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=0.005...
{'loss': 0.0035, 'grad_norm': 0.4656165540218353, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.299614191055298, 'eval_runtime': 7.1427, 'eval_samples_per_second': 12.18, 'eval_steps_per_second': 0.84, 'epoch': 0.22}
{'loss': 0.0108, 'grad_norm': 0.2159292995929718, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.300898790359497, 'eval_runtime': 7.1947, 'eval_samples_per_second': 12.092, 'eval_steps_per_second': 0.834, 'epoch': 0.44}
{'loss': 0.0141, 'grad_norm': 0.3885461390018463, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.302762031555176, 'eval_runtime': 7.2002, 'eval_samples_per_second': 12.083, 'eval_steps_per_second': 0.833, 'epoch': 0.67}
{'loss': 0.0179, 'grad_norm': 0.3207864463329315, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.30291748046875, 'eval_runtime': 7.171, 'eval_samples_per_second': 12.132, 'eval_steps_per_second': 0.837, 'epoch': 0.89}
{'loss': 0.0182, 'grad_norm': 0.2672010362148285, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.3000969886779785, 'eval_runtime': 7.2015, 'eval_samples_per_second': 12.081, 'eval_steps_per_second': 0.833, 'epoch': 1.11}
{'loss': 0.0183, 'grad_norm': 0.2624114155769348, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.300741195678711, 'eval_runtime': 7.2049, 'eval_samples_per_second': 12.075, 'eval_steps_per_second': 0.833, 'epoch': 1.33}
{'loss': 0.0182, 'grad_norm': 0.30200546979904175, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:21<08:05,  3.59s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 40%|████████████████████████████████████████████████████████▍                                                                                    | 90/225 [06:21<08:05,  3.59s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'loss': 0.0194, 'grad_norm': 0.41334372758865356, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.302077054977417, 'eval_runtime': 7.2105, 'eval_samples_per_second': 12.066, 'eval_steps_per_second': 0.832, 'epoch': 1.78}
{'loss': 0.0213, 'grad_norm': 0.4028508961200714, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.30411958694458, 'eval_runtime': 7.2377, 'eval_samples_per_second': 12.02, 'eval_steps_per_second': 0.829, 'epoch': 2.0}
{'loss': 0.024, 'grad_norm': 0.4305880665779114, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.30448579788208, 'eval_runtime': 7.255, 'eval_samples_per_second': 11.992, 'eval_steps_per_second': 0.827, 'epoch': 2.22}
{'loss': 0.025, 'grad_norm': 0.3935757875442505, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.302311420440674, 'eval_runtime': 7.2938, 'eval_samples_per_second': 11.928, 'eval_steps_per_second': 0.823, 'epoch': 2.44}
{'loss': 0.0284, 'grad_norm': 0.4258618652820587, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.3011860847473145, 'eval_runtime': 7.259, 'eval_samples_per_second': 11.985, 'eval_steps_per_second': 0.827, 'epoch': 2.67}
{'loss': 0.0352, 'grad_norm': 0.488864541053772, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.301168441772461, 'eval_runtime': 7.2769, 'eval_samples_per_second': 11.956, 'eval_steps_per_second': 0.825, 'epoch': 2.89}
{'loss': 0.0365, 'grad_norm': 0.4050625264644623, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.2990810871124268, 'eval_runtime': 7.2667, 'eval_samples_per_second': 11.972, 'eval_steps_per_second': 0.826, 'epoch': 3.11}
{'loss': 0.0405, 'grad_norm': 0.4072140157222748, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.297769546508789, 'eval_runtime': 7.2345, 'eval_samples_per_second': 12.026, 'eval_steps_per_second': 0.829, 'epoch': 3.33}
{'loss': 0.0437, 'grad_norm': 0.43889668583869934, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
{'eval_loss': 3.2970051765441895, 'eval_runtime': 7.2744, 'eval_samples_per_second': 11.96, 'eval_steps_per_second': 0.825, 'epoch': 3.56}
{'loss': 0.0503, 'grad_norm': 0.4363868832588196, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.296071767807007, 'eval_runtime': 7.2731, 'eval_samples_per_second': 11.962, 'eval_steps_per_second': 0.825, 'epoch': 3.78}
{'loss': 0.0558, 'grad_norm': 0.4623761773109436, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.294402837753296, 'eval_runtime': 7.2918, 'eval_samples_per_second': 11.931, 'eval_steps_per_second': 0.823, 'epoch': 4.0}
{'loss': 0.0636, 'grad_norm': 0.6699292063713074, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.293168306350708, 'eval_runtime': 7.2699, 'eval_samples_per_second': 11.967, 'eval_steps_per_second': 0.825, 'epoch': 4.22}
{'loss': 0.0757, 'grad_norm': 0.9292557835578918, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.292407751083374, 'eval_runtime': 7.2553, 'eval_samples_per_second': 11.991, 'eval_steps_per_second': 0.827, 'epoch': 4.44}
{'loss': 0.0891, 'grad_norm': 0.7105364799499512, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.2915194034576416, 'eval_runtime': 7.2865, 'eval_samples_per_second': 11.94, 'eval_steps_per_second': 0.823, 'epoch': 4.67}
{'loss': 0.1053, 'grad_norm': 0.8960104584693909, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
{'eval_loss': 3.2909040451049805, 'eval_runtime': 7.2696, 'eval_samples_per_second': 11.968, 'eval_steps_per_second': 0.825, 'epoch': 4.89}
{'train_runtime': 947.6199, 'train_samples_per_second': 3.799, 'train_steps_per_second': 0.237, 'train_loss': 0.03900592800643709, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd0.005...
Fine-tuning for learning_rate=1e-05 and weight_decay=0.005 complete. Model saved to './LlamaFinetuned_lr1e-05_wd0.005'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=0.001...
{'loss': 0.0034, 'grad_norm': 0.4599078297615051, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.302933692932129, 'eval_runtime': 7.2599, 'eval_samples_per_second': 11.984, 'eval_steps_per_second': 0.826, 'epoch': 0.22}
{'loss': 0.0104, 'grad_norm': 0.21715210378170013, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.304018497467041, 'eval_runtime': 7.2752, 'eval_samples_per_second': 11.958, 'eval_steps_per_second': 0.825, 'epoch': 0.44}
{'loss': 0.0137, 'grad_norm': 0.3868824541568756, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.3057639598846436, 'eval_runtime': 7.3033, 'eval_samples_per_second': 11.912, 'eval_steps_per_second': 0.822, 'epoch': 0.67}
{'loss': 0.0174, 'grad_norm': 0.31757795810699463, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.3057780265808105, 'eval_runtime': 7.3079, 'eval_samples_per_second': 11.905, 'eval_steps_per_second': 0.821, 'epoch': 0.89}
{'loss': 0.0178, 'grad_norm': 0.26408955454826355, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.3028347492218018, 'eval_runtime': 7.2846, 'eval_samples_per_second': 11.943, 'eval_steps_per_second': 0.824, 'epoch': 1.11}
{'loss': 0.018, 'grad_norm': 0.269212007522583, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.303417444229126, 'eval_runtime': 7.296, 'eval_samples_per_second': 11.924, 'eval_steps_per_second': 0.822, 'epoch': 1.33}
{'loss': 0.0179, 'grad_norm': 0.3051271438598633, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
{'eval_loss': 3.304356336593628, 'eval_runtime': 7.2555, 'eval_samples_per_second': 11.991, 'eval_steps_per_second': 0.827, 'epoch': 1.56}
{'loss': 0.0191, 'grad_norm': 0.4177377223968506, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.3045973777770996, 'eval_runtime': 7.238, 'eval_samples_per_second': 12.02, 'eval_steps_per_second': 0.829, 'epoch': 1.78}
{'loss': 0.021, 'grad_norm': 0.40479615330696106, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.306624412536621, 'eval_runtime': 7.2708, 'eval_samples_per_second': 11.966, 'eval_steps_per_second': 0.825, 'epoch': 2.0}
{'loss': 0.0237, 'grad_norm': 0.43666449189186096, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.306955337524414, 'eval_runtime': 7.2542, 'eval_samples_per_second': 11.993, 'eval_steps_per_second': 0.827, 'epoch': 2.22}
{'loss': 0.0248, 'grad_norm': 0.39763373136520386, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.304807424545288, 'eval_runtime': 7.2602, 'eval_samples_per_second': 11.983, 'eval_steps_per_second': 0.826, 'epoch': 2.44}
{'loss': 0.0282, 'grad_norm': 0.4285070598125458, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.303602933883667, 'eval_runtime': 7.282, 'eval_samples_per_second': 11.947, 'eval_steps_per_second': 0.824, 'epoch': 2.67}
{'loss': 0.0351, 'grad_norm': 0.49554258584976196, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.303523302078247, 'eval_runtime': 7.2564, 'eval_samples_per_second': 11.989, 'eval_steps_per_second': 0.827, 'epoch': 2.89}
{'loss': 0.0365, 'grad_norm': 0.4084107279777527, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.3014237880706787, 'eval_runtime': 7.2653, 'eval_samples_per_second': 11.975, 'eval_steps_per_second': 0.826, 'epoch': 3.11}
{'loss': 0.0405, 'grad_norm': 0.4101830720901489, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.3000850677490234, 'eval_runtime': 7.2617, 'eval_samples_per_second': 11.981, 'eval_steps_per_second': 0.826, 'epoch': 3.33}
{'loss': 0.0438, 'grad_norm': 0.4431401193141937, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
{'eval_loss': 3.299335241317749, 'eval_runtime': 7.2994, 'eval_samples_per_second': 11.919, 'eval_steps_per_second': 0.822, 'epoch': 3.56}
{'loss': 0.0504, 'grad_norm': 0.4442327916622162, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.2983946800231934, 'eval_runtime': 7.2951, 'eval_samples_per_second': 11.926, 'eval_steps_per_second': 0.822, 'epoch': 3.78}
{'loss': 0.056, 'grad_norm': 0.470290869474411, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.296659231185913, 'eval_runtime': 7.295, 'eval_samples_per_second': 11.926, 'eval_steps_per_second': 0.822, 'epoch': 4.0}
{'loss': 0.0638, 'grad_norm': 0.6749331951141357, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.2953708171844482, 'eval_runtime': 7.2734, 'eval_samples_per_second': 11.961, 'eval_steps_per_second': 0.825, 'epoch': 4.22}
{'loss': 0.076, 'grad_norm': 0.94253009557724, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.29459285736084, 'eval_runtime': 7.3151, 'eval_samples_per_second': 11.893, 'eval_steps_per_second': 0.82, 'epoch': 4.44}
{'loss': 0.0895, 'grad_norm': 0.7204350829124451, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.293692111968994, 'eval_runtime': 7.2626, 'eval_samples_per_second': 11.979, 'eval_steps_per_second': 0.826, 'epoch': 4.67}
{'loss': 0.1058, 'grad_norm': 0.9058824181556702, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
{'eval_loss': 3.293069839477539, 'eval_runtime': 7.2943, 'eval_samples_per_second': 11.927, 'eval_steps_per_second': 0.823, 'epoch': 4.89}
{'train_runtime': 954.3575, 'train_samples_per_second': 3.772, 'train_steps_per_second': 0.236, 'train_loss': 0.03891886133286688, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd0.001...
Fine-tuning for learning_rate=1e-05 and weight_decay=0.001 complete. Model saved to './LlamaFinetuned_lr1e-05_wd0.001'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=0.0005...
{'loss': 0.0032, 'grad_norm': 0.4523698091506958, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.3051743507385254, 'eval_runtime': 7.2515, 'eval_samples_per_second': 11.997, 'eval_steps_per_second': 0.827, 'epoch': 0.22}
{'loss': 0.01, 'grad_norm': 0.21724356710910797, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.3061890602111816, 'eval_runtime': 7.2668, 'eval_samples_per_second': 11.972, 'eval_steps_per_second': 0.826, 'epoch': 0.44}
{'loss': 0.0133, 'grad_norm': 0.3842984437942505, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.3078694343566895, 'eval_runtime': 7.2457, 'eval_samples_per_second': 12.007, 'eval_steps_per_second': 0.828, 'epoch': 0.67}
{'loss': 0.017, 'grad_norm': 0.3142520785331726, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.3077445030212402, 'eval_runtime': 7.2862, 'eval_samples_per_second': 11.94, 'eval_steps_per_second': 0.823, 'epoch': 0.89}
{'loss': 0.0175, 'grad_norm': 0.2599632740020752, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.304703950881958, 'eval_runtime': 7.281, 'eval_samples_per_second': 11.949, 'eval_steps_per_second': 0.824, 'epoch': 1.11}
{'loss': 0.0177, 'grad_norm': 0.2747538983821869, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.305258274078369, 'eval_runtime': 7.273, 'eval_samples_per_second': 11.962, 'eval_steps_per_second': 0.825, 'epoch': 1.33}
{'loss': 0.0177, 'grad_norm': 0.3069809675216675, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
{'eval_loss': 3.3061330318450928, 'eval_runtime': 7.2713, 'eval_samples_per_second': 11.965, 'eval_steps_per_second': 0.825, 'epoch': 1.56}
{'loss': 0.0189, 'grad_norm': 0.4207654595375061, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.3063406944274902, 'eval_runtime': 7.2542, 'eval_samples_per_second': 11.993, 'eval_steps_per_second': 0.827, 'epoch': 1.78}
{'loss': 0.0208, 'grad_norm': 0.4066961705684662, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.308366537094116, 'eval_runtime': 7.2798, 'eval_samples_per_second': 11.951, 'eval_steps_per_second': 0.824, 'epoch': 2.0}
{'loss': 0.0235, 'grad_norm': 0.44069337844848633, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.308690309524536, 'eval_runtime': 7.252, 'eval_samples_per_second': 11.997, 'eval_steps_per_second': 0.827, 'epoch': 2.22}
{'loss': 0.0246, 'grad_norm': 0.4010917842388153, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.3066458702087402, 'eval_runtime': 7.3017, 'eval_samples_per_second': 11.915, 'eval_steps_per_second': 0.822, 'epoch': 2.44}
{'loss': 0.0281, 'grad_norm': 0.42888885736465454, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.3053810596466064, 'eval_runtime': 7.2692, 'eval_samples_per_second': 11.968, 'eval_steps_per_second': 0.825, 'epoch': 2.67}
{'loss': 0.035, 'grad_norm': 0.5014772415161133, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.305267572402954, 'eval_runtime': 7.2863, 'eval_samples_per_second': 11.94, 'eval_steps_per_second': 0.823, 'epoch': 2.89}
{'loss': 0.0364, 'grad_norm': 0.4109029471874237, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.3031692504882812, 'eval_runtime': 7.2985, 'eval_samples_per_second': 11.92, 'eval_steps_per_second': 0.822, 'epoch': 3.11}
{'loss': 0.0404, 'grad_norm': 0.41282641887664795, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.3018078804016113, 'eval_runtime': 7.2896, 'eval_samples_per_second': 11.935, 'eval_steps_per_second': 0.823, 'epoch': 3.33}
{'loss': 0.0438, 'grad_norm': 0.44662806391716003, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
{'eval_loss': 3.301076889038086, 'eval_runtime': 7.2915, 'eval_samples_per_second': 11.932, 'eval_steps_per_second': 0.823, 'epoch': 3.56}
{'loss': 0.0505, 'grad_norm': 0.451238214969635, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.3001320362091064, 'eval_runtime': 7.2726, 'eval_samples_per_second': 11.963, 'eval_steps_per_second': 0.825, 'epoch': 3.78}
{'loss': 0.0561, 'grad_norm': 0.4766339659690857, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.2983310222625732, 'eval_runtime': 7.2928, 'eval_samples_per_second': 11.93, 'eval_steps_per_second': 0.823, 'epoch': 4.0}
{'loss': 0.0639, 'grad_norm': 0.6797250509262085, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.29697847366333, 'eval_runtime': 7.2658, 'eval_samples_per_second': 11.974, 'eval_steps_per_second': 0.826, 'epoch': 4.22}
{'loss': 0.0762, 'grad_norm': 0.9532915949821472, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.2961766719818115, 'eval_runtime': 7.2687, 'eval_samples_per_second': 11.969, 'eval_steps_per_second': 0.825, 'epoch': 4.44}
{'loss': 0.0898, 'grad_norm': 0.7290698885917664, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.295264482498169, 'eval_runtime': 7.2647, 'eval_samples_per_second': 11.976, 'eval_steps_per_second': 0.826, 'epoch': 4.67}
{'loss': 0.1061, 'grad_norm': 0.9142794013023376, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:43<02:41,  3.60s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 180/225 [12:43<02:41,  3.60s/it]/home/jawadkk/.conda/envs/vEnv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
{'train_runtime': 954.7754, 'train_samples_per_second': 3.771, 'train_steps_per_second': 0.236, 'train_loss': 0.03882436426149474, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd0.0005...
Fine-tuning for learning_rate=1e-05 and weight_decay=0.0005 complete. Model saved to './LlamaFinetuned_lr1e-05_wd0.0005'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=0.0001...
{'loss': 0.0031, 'grad_norm': 0.4436374306678772, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.3068220615386963, 'eval_runtime': 7.2464, 'eval_samples_per_second': 12.006, 'eval_steps_per_second': 0.828, 'epoch': 0.22}
{'loss': 0.0097, 'grad_norm': 0.21653346717357635, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.307858467102051, 'eval_runtime': 7.2689, 'eval_samples_per_second': 11.969, 'eval_steps_per_second': 0.825, 'epoch': 0.44}
{'loss': 0.013, 'grad_norm': 0.3815796375274658, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.309506893157959, 'eval_runtime': 7.2603, 'eval_samples_per_second': 11.983, 'eval_steps_per_second': 0.826, 'epoch': 0.67}
{'loss': 0.0167, 'grad_norm': 0.31103482842445374, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.3092315196990967, 'eval_runtime': 7.2834, 'eval_samples_per_second': 11.945, 'eval_steps_per_second': 0.824, 'epoch': 0.89}
{'loss': 0.0171, 'grad_norm': 0.25552746653556824, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.3061046600341797, 'eval_runtime': 7.277, 'eval_samples_per_second': 11.955, 'eval_steps_per_second': 0.825, 'epoch': 1.11}
{'loss': 0.0174, 'grad_norm': 0.2796158492565155, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.3066415786743164, 'eval_runtime': 7.2781, 'eval_samples_per_second': 11.954, 'eval_steps_per_second': 0.824, 'epoch': 1.33}
{'loss': 0.0174, 'grad_norm': 0.3080199658870697, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
{'eval_loss': 3.3074684143066406, 'eval_runtime': 7.3081, 'eval_samples_per_second': 11.905, 'eval_steps_per_second': 0.821, 'epoch': 1.56}
{'loss': 0.0186, 'grad_norm': 0.4231070876121521, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.307661771774292, 'eval_runtime': 7.2928, 'eval_samples_per_second': 11.93, 'eval_steps_per_second': 0.823, 'epoch': 1.78}
{'loss': 0.0205, 'grad_norm': 0.4087144732475281, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.3096909523010254, 'eval_runtime': 7.301, 'eval_samples_per_second': 11.916, 'eval_steps_per_second': 0.822, 'epoch': 2.0}
{'loss': 0.0232, 'grad_norm': 0.4438643455505371, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.310030460357666, 'eval_runtime': 7.2988, 'eval_samples_per_second': 11.92, 'eval_steps_per_second': 0.822, 'epoch': 2.22}
{'loss': 0.0244, 'grad_norm': 0.4043702483177185, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.308114528656006, 'eval_runtime': 7.2955, 'eval_samples_per_second': 11.925, 'eval_steps_per_second': 0.822, 'epoch': 2.44}
{'loss': 0.0279, 'grad_norm': 0.4279305636882782, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.306804895401001, 'eval_runtime': 7.2607, 'eval_samples_per_second': 11.982, 'eval_steps_per_second': 0.826, 'epoch': 2.67}
{'loss': 0.0349, 'grad_norm': 0.5071631669998169, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.306657552719116, 'eval_runtime': 7.2689, 'eval_samples_per_second': 11.969, 'eval_steps_per_second': 0.825, 'epoch': 2.89}
{'loss': 0.0363, 'grad_norm': 0.41313186287879944, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.3045542240142822, 'eval_runtime': 7.2894, 'eval_samples_per_second': 11.935, 'eval_steps_per_second': 0.823, 'epoch': 3.11}
{'loss': 0.0404, 'grad_norm': 0.41536012291908264, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.303184747695923, 'eval_runtime': 7.2831, 'eval_samples_per_second': 11.945, 'eval_steps_per_second': 0.824, 'epoch': 3.33}
{'loss': 0.0438, 'grad_norm': 0.44982245564460754, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
{'eval_loss': 3.302455425262451, 'eval_runtime': 7.2986, 'eval_samples_per_second': 11.92, 'eval_steps_per_second': 0.822, 'epoch': 3.56}
{'loss': 0.0506, 'grad_norm': 0.45784375071525574, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.301511526107788, 'eval_runtime': 7.3037, 'eval_samples_per_second': 11.912, 'eval_steps_per_second': 0.822, 'epoch': 3.78}
{'loss': 0.0561, 'grad_norm': 0.4820865988731384, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.299668550491333, 'eval_runtime': 7.2737, 'eval_samples_per_second': 11.961, 'eval_steps_per_second': 0.825, 'epoch': 4.0}
{'loss': 0.064, 'grad_norm': 0.6843770146369934, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.2982540130615234, 'eval_runtime': 7.2529, 'eval_samples_per_second': 11.995, 'eval_steps_per_second': 0.827, 'epoch': 4.22}
{'loss': 0.0764, 'grad_norm': 0.962623119354248, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.297428846359253, 'eval_runtime': 7.2571, 'eval_samples_per_second': 11.988, 'eval_steps_per_second': 0.827, 'epoch': 4.44}
{'loss': 0.09, 'grad_norm': 0.7370007038116455, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.296501874923706, 'eval_runtime': 7.2726, 'eval_samples_per_second': 11.963, 'eval_steps_per_second': 0.825, 'epoch': 4.67}
{'loss': 0.1065, 'grad_norm': 0.9218270778656006, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
{'eval_loss': 3.295863389968872, 'eval_runtime': 7.2571, 'eval_samples_per_second': 11.988, 'eval_steps_per_second': 0.827, 'epoch': 4.89}
{'train_runtime': 954.4508, 'train_samples_per_second': 3.772, 'train_steps_per_second': 0.236, 'train_loss': 0.038725583205620445, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd0.0001...
Fine-tuning for learning_rate=1e-05 and weight_decay=0.0001 complete. Model saved to './LlamaFinetuned_lr1e-05_wd0.0001'.
Starting fine-tuning for learning_rate=1e-05 and weight_decay=1e-05...
{'loss': 0.003, 'grad_norm': 0.4341089725494385, 'learning_rate': 9.555555555555556e-06, 'epoch': 0.22}
{'eval_loss': 3.3081233501434326, 'eval_runtime': 7.2612, 'eval_samples_per_second': 11.981, 'eval_steps_per_second': 0.826, 'epoch': 0.22}
{'loss': 0.0094, 'grad_norm': 0.2151966691017151, 'learning_rate': 9.111111111111112e-06, 'epoch': 0.44}
{'eval_loss': 3.3092398643493652, 'eval_runtime': 7.2865, 'eval_samples_per_second': 11.94, 'eval_steps_per_second': 0.823, 'epoch': 0.44}
{'loss': 0.0127, 'grad_norm': 0.37888213992118835, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.67}
{'eval_loss': 3.310879707336426, 'eval_runtime': 7.2745, 'eval_samples_per_second': 11.96, 'eval_steps_per_second': 0.825, 'epoch': 0.67}
{'loss': 0.0163, 'grad_norm': 0.3079068064689636, 'learning_rate': 8.222222222222222e-06, 'epoch': 0.89}
{'eval_loss': 3.3104472160339355, 'eval_runtime': 7.261, 'eval_samples_per_second': 11.982, 'eval_steps_per_second': 0.826, 'epoch': 0.89}
{'loss': 0.0168, 'grad_norm': 0.2510295808315277, 'learning_rate': 7.77777777777778e-06, 'epoch': 1.11}
{'eval_loss': 3.3072288036346436, 'eval_runtime': 7.2976, 'eval_samples_per_second': 11.922, 'eval_steps_per_second': 0.822, 'epoch': 1.11}
{'loss': 0.0172, 'grad_norm': 0.28404733538627625, 'learning_rate': 7.333333333333333e-06, 'epoch': 1.33}
{'eval_loss': 3.3077521324157715, 'eval_runtime': 7.3069, 'eval_samples_per_second': 11.907, 'eval_steps_per_second': 0.821, 'epoch': 1.33}
{'loss': 0.0172, 'grad_norm': 0.30849164724349976, 'learning_rate': 6.88888888888889e-06, 'epoch': 1.56}
{'eval_loss': 3.3085410594940186, 'eval_runtime': 7.2688, 'eval_samples_per_second': 11.969, 'eval_steps_per_second': 0.825, 'epoch': 1.56}
{'loss': 0.0184, 'grad_norm': 0.42504072189331055, 'learning_rate': 6.444444444444445e-06, 'epoch': 1.78}
{'eval_loss': 3.308739423751831, 'eval_runtime': 7.2848, 'eval_samples_per_second': 11.943, 'eval_steps_per_second': 0.824, 'epoch': 1.78}
{'loss': 0.0203, 'grad_norm': 0.41081562638282776, 'learning_rate': 6e-06, 'epoch': 2.0}
{'eval_loss': 3.310781955718994, 'eval_runtime': 7.267, 'eval_samples_per_second': 11.972, 'eval_steps_per_second': 0.826, 'epoch': 2.0}
{'loss': 0.023, 'grad_norm': 0.4466189444065094, 'learning_rate': 5.555555555555557e-06, 'epoch': 2.22}
{'eval_loss': 3.311147451400757, 'eval_runtime': 7.2688, 'eval_samples_per_second': 11.969, 'eval_steps_per_second': 0.825, 'epoch': 2.22}
{'loss': 0.0242, 'grad_norm': 0.4076324701309204, 'learning_rate': 5.1111111111111115e-06, 'epoch': 2.44}
{'eval_loss': 3.3093464374542236, 'eval_runtime': 7.3111, 'eval_samples_per_second': 11.9, 'eval_steps_per_second': 0.821, 'epoch': 2.44}
{'loss': 0.0277, 'grad_norm': 0.42607152462005615, 'learning_rate': 4.666666666666667e-06, 'epoch': 2.67}
{'eval_loss': 3.3080086708068848, 'eval_runtime': 7.2869, 'eval_samples_per_second': 11.939, 'eval_steps_per_second': 0.823, 'epoch': 2.67}
{'loss': 0.0348, 'grad_norm': 0.5126989483833313, 'learning_rate': 4.222222222222223e-06, 'epoch': 2.89}
{'eval_loss': 3.307811975479126, 'eval_runtime': 7.2885, 'eval_samples_per_second': 11.937, 'eval_steps_per_second': 0.823, 'epoch': 2.89}
{'loss': 0.0361, 'grad_norm': 0.4153064787387848, 'learning_rate': 3.777777777777778e-06, 'epoch': 3.11}
{'eval_loss': 3.305699586868286, 'eval_runtime': 7.255, 'eval_samples_per_second': 11.992, 'eval_steps_per_second': 0.827, 'epoch': 3.11}
{'loss': 0.0403, 'grad_norm': 0.41792210936546326, 'learning_rate': 3.3333333333333333e-06, 'epoch': 3.33}
{'eval_loss': 3.3043415546417236, 'eval_runtime': 7.2765, 'eval_samples_per_second': 11.956, 'eval_steps_per_second': 0.825, 'epoch': 3.33}
{'loss': 0.0437, 'grad_norm': 0.45290178060531616, 'learning_rate': 2.888888888888889e-06, 'epoch': 3.56}
{'eval_loss': 3.3035948276519775, 'eval_runtime': 7.2795, 'eval_samples_per_second': 11.951, 'eval_steps_per_second': 0.824, 'epoch': 3.56}
{'loss': 0.0506, 'grad_norm': 0.46419447660446167, 'learning_rate': 2.4444444444444447e-06, 'epoch': 3.78}
{'eval_loss': 3.302659273147583, 'eval_runtime': 7.2771, 'eval_samples_per_second': 11.955, 'eval_steps_per_second': 0.825, 'epoch': 3.78}
{'loss': 0.0562, 'grad_norm': 0.48694732785224915, 'learning_rate': 2.0000000000000003e-06, 'epoch': 4.0}
{'eval_loss': 3.300816297531128, 'eval_runtime': 7.2927, 'eval_samples_per_second': 11.93, 'eval_steps_per_second': 0.823, 'epoch': 4.0}
{'loss': 0.0641, 'grad_norm': 0.6890132427215576, 'learning_rate': 1.5555555555555558e-06, 'epoch': 4.22}
{'eval_loss': 3.299342632293701, 'eval_runtime': 7.27, 'eval_samples_per_second': 11.967, 'eval_steps_per_second': 0.825, 'epoch': 4.22}
{'loss': 0.0766, 'grad_norm': 0.9709957838058472, 'learning_rate': 1.111111111111111e-06, 'epoch': 4.44}
{'eval_loss': 3.2984893321990967, 'eval_runtime': 7.2821, 'eval_samples_per_second': 11.947, 'eval_steps_per_second': 0.824, 'epoch': 4.44}
{'loss': 0.0903, 'grad_norm': 0.7444795370101929, 'learning_rate': 6.666666666666667e-07, 'epoch': 4.67}
{'eval_loss': 3.2975473403930664, 'eval_runtime': 7.2692, 'eval_samples_per_second': 11.968, 'eval_steps_per_second': 0.825, 'epoch': 4.67}
{'loss': 0.1068, 'grad_norm': 0.9288859367370605, 'learning_rate': 2.2222222222222224e-07, 'epoch': 4.89}
{'eval_loss': 3.2969024181365967, 'eval_runtime': 7.2448, 'eval_samples_per_second': 12.009, 'eval_steps_per_second': 0.828, 'epoch': 4.89}
{'train_runtime': 953.4981, 'train_samples_per_second': 3.776, 'train_steps_per_second': 0.236, 'train_loss': 0.03862423815661006, 'epoch': 5.0}
Saving the fine-tuned model to ./LlamaFinetuned_lr1e-05_wd1e-05...
Fine-tuning for learning_rate=1e-05 and weight_decay=1e-05 complete. Model saved to './LlamaFinetuned_lr1e-05_wd1e-05'.
All fine-tuning tasks completed.